{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classification Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCRIPTON\n",
    "----------\n",
    "\n",
    "This dataset is composed of 1300 samples with 31 features each. The first column\n",
    "is the sample id. The second column in the dataset represents the label. There\n",
    "are 5 possible values for the labels. The remaining columns are numeric\n",
    "features, except for the last column which is categorical (with 3 categories).\n",
    "\n",
    "Notice that the classes are unbalanced: some labels are more frequent than\n",
    "others. You need to decide whether to take this into account, and if so how.\n",
    "\n",
    "Your task is the following: you should compare the performance of Logistic\n",
    "Regression (implemented by sklearn.linear_model.LogisticRegression) with that of\n",
    "a Random Forest (implemented by sklearn.ensemble.ExtraTreesClassifier). Try to\n",
    "optimize both algorithms' parameters and determine which one is best for this\n",
    "dataset. At the end of the analysis, you should have chosen an algorithm and its\n",
    "optimal set of parameters: write this choice explicitly in the conclusions of\n",
    "your notebook.\n",
    "\n",
    "Your notebook should detail the procedure you have used to choose the optimal\n",
    "parameters (graphs are a good idea when possible/sensible).\n",
    "\n",
    "The notebook will be evaluated not only based on the final results, but also on\n",
    "the procedure employed, which should balance practical considerations (one may\n",
    "not be able to exhaustively explore all possible combinations of the parameters)\n",
    "with the desire for achieving the best possible performance in the least amount\n",
    "of time.\n",
    "\n",
    "Bonus points may be assigned for particularly clean/nifty code and/or well-\n",
    "presented results.\n",
    "\n",
    "You are also free to attempt other strategies beyond the one in the assignment\n",
    "(which however is mandatory!).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted from the description above, we have a dataset of 1300 samples with 31 features each, including a column containing categorical data. Our task is to perform a classification analysis on this dataset comparing two main algorithms - Logistic Regression & Random Forest. Our pipeline will compose of the following key stages: \n",
    "\n",
    "1) Importing the relevant libraries and the main dataset.\n",
    "\n",
    "2) Data Preprocessing (where we will ensure we don't have any missing data, encode our categorical features and perform feature scaling).\n",
    "\n",
    "3) Implementation of the classification algorithm and base model - Logistic Regression.\n",
    "\n",
    "4) Evaluation of the first base model - applying K-Fold Cross Validation.\n",
    "\n",
    "5) Implemention of the second classification algorithm (Random Forest)\n",
    "\n",
    "6) Evaluation of the second model - applying K-Fold Cross Validation.\n",
    "\n",
    "7) Applying Grid-Search to find the most optimal set of parameters. \n",
    "\n",
    "9) Further application of techniques to optimize both models.\n",
    "\n",
    "10) Final selection of the most optimal model and conclusion of analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>categorical_feature_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.623405</td>\n",
       "      <td>1.157472</td>\n",
       "      <td>0.978446</td>\n",
       "      <td>3.951788</td>\n",
       "      <td>0.918710</td>\n",
       "      <td>-0.742102</td>\n",
       "      <td>-1.764838</td>\n",
       "      <td>1.959709</td>\n",
       "      <td>-0.430550</td>\n",
       "      <td>...</td>\n",
       "      <td>1.245197</td>\n",
       "      <td>-0.349880</td>\n",
       "      <td>1.193921</td>\n",
       "      <td>-1.764838</td>\n",
       "      <td>-0.030706</td>\n",
       "      <td>-1.376937</td>\n",
       "      <td>-0.876525</td>\n",
       "      <td>-1.303575</td>\n",
       "      <td>-0.685853</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.017604</td>\n",
       "      <td>0.962947</td>\n",
       "      <td>1.971217</td>\n",
       "      <td>-2.183554</td>\n",
       "      <td>-2.475973</td>\n",
       "      <td>-4.056342</td>\n",
       "      <td>0.148193</td>\n",
       "      <td>0.529200</td>\n",
       "      <td>-0.692605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.266497</td>\n",
       "      <td>1.327070</td>\n",
       "      <td>1.646004</td>\n",
       "      <td>0.148193</td>\n",
       "      <td>-4.494677</td>\n",
       "      <td>-0.890755</td>\n",
       "      <td>-0.290291</td>\n",
       "      <td>-0.411538</td>\n",
       "      <td>-2.506186</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.814985</td>\n",
       "      <td>-2.619374</td>\n",
       "      <td>-0.777541</td>\n",
       "      <td>1.921058</td>\n",
       "      <td>-0.550762</td>\n",
       "      <td>1.097364</td>\n",
       "      <td>0.453402</td>\n",
       "      <td>0.426274</td>\n",
       "      <td>1.081457</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.147455</td>\n",
       "      <td>-0.289827</td>\n",
       "      <td>0.043222</td>\n",
       "      <td>0.453402</td>\n",
       "      <td>1.512044</td>\n",
       "      <td>0.193947</td>\n",
       "      <td>0.273527</td>\n",
       "      <td>0.191259</td>\n",
       "      <td>-0.336440</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.253860</td>\n",
       "      <td>0.276528</td>\n",
       "      <td>1.124507</td>\n",
       "      <td>-0.522192</td>\n",
       "      <td>1.223016</td>\n",
       "      <td>-0.521033</td>\n",
       "      <td>1.267149</td>\n",
       "      <td>0.400720</td>\n",
       "      <td>-0.480437</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.490817</td>\n",
       "      <td>0.365089</td>\n",
       "      <td>-0.612431</td>\n",
       "      <td>1.267149</td>\n",
       "      <td>0.385040</td>\n",
       "      <td>-0.317562</td>\n",
       "      <td>0.570157</td>\n",
       "      <td>0.224122</td>\n",
       "      <td>0.808667</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.119872</td>\n",
       "      <td>0.293963</td>\n",
       "      <td>0.421747</td>\n",
       "      <td>4.960528</td>\n",
       "      <td>-1.371113</td>\n",
       "      <td>-1.611126</td>\n",
       "      <td>-0.271988</td>\n",
       "      <td>3.291267</td>\n",
       "      <td>0.329875</td>\n",
       "      <td>...</td>\n",
       "      <td>1.838477</td>\n",
       "      <td>1.946403</td>\n",
       "      <td>0.495256</td>\n",
       "      <td>-0.271988</td>\n",
       "      <td>-1.412096</td>\n",
       "      <td>0.891830</td>\n",
       "      <td>1.858374</td>\n",
       "      <td>0.998611</td>\n",
       "      <td>0.720807</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0      4  -0.623405   1.157472   0.978446   3.951788   0.918710  -0.742102   \n",
       "1      1  -3.017604   0.962947   1.971217  -2.183554  -2.475973  -4.056342   \n",
       "2      1  -1.814985  -2.619374  -0.777541   1.921058  -0.550762   1.097364   \n",
       "3      3   0.253860   0.276528   1.124507  -0.522192   1.223016  -0.521033   \n",
       "4      4  -1.119872   0.293963   0.421747   4.960528  -1.371113  -1.611126   \n",
       "\n",
       "   feature_7  feature_8  feature_9  ...  feature_22  feature_23  feature_24  \\\n",
       "0  -1.764838   1.959709  -0.430550  ...    1.245197   -0.349880    1.193921   \n",
       "1   0.148193   0.529200  -0.692605  ...   -0.266497    1.327070    1.646004   \n",
       "2   0.453402   0.426274   1.081457  ...   -1.147455   -0.289827    0.043222   \n",
       "3   1.267149   0.400720  -0.480437  ...   -0.490817    0.365089   -0.612431   \n",
       "4  -0.271988   3.291267   0.329875  ...    1.838477    1.946403    0.495256   \n",
       "\n",
       "   feature_25  feature_26  feature_27  feature_28  feature_29  feature_30  \\\n",
       "0   -1.764838   -0.030706   -1.376937   -0.876525   -1.303575   -0.685853   \n",
       "1    0.148193   -4.494677   -0.890755   -0.290291   -0.411538   -2.506186   \n",
       "2    0.453402    1.512044    0.193947    0.273527    0.191259   -0.336440   \n",
       "3    1.267149    0.385040   -0.317562    0.570157    0.224122    0.808667   \n",
       "4   -0.271988   -1.412096    0.891830    1.858374    0.998611    0.720807   \n",
       "\n",
       "   categorical_feature_1  \n",
       "0                      C  \n",
       "1                      A  \n",
       "2                      A  \n",
       "3                      B  \n",
       "4                      C  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/Users/aryan/Desktop/mldata_0013072150') \n",
    "dataset = pd.read_csv('mldata_0013072150.csv')\n",
    "dataset = dataset.drop(\"Unnamed: 0\", axis=1) #To remove the \"Unnamed: 0\" column to make the table more readable. \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only judging by the first five rows, we can already notice by the \"label\" column, that this is not a binary classification problem. Thus, we will use the groupby() method to check how many classes we actually have and the number of instances of each class in order to check if our classes are balanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    280\n",
       "1    458\n",
       "2     94\n",
       "3    188\n",
       "4    280\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby('label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we have unbalanced classes. Just from raw examination, it seems like the class with the label '1' is the majority class. For now, we note this observation and progress to the Data Preprocessing stage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to ensure if we have any missing data as this can cause problems further down the pipeline, resulting in unwanted problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>categorical_feature_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [label, feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7, feature_8, feature_9, feature_10, feature_11, feature_12, feature_13, feature_14, feature_15, feature_16, feature_17, feature_18, feature_19, feature_20, feature_21, feature_22, feature_23, feature_24, feature_25, feature_26, feature_27, feature_28, feature_29, feature_30, categorical_feature_1]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_incomplete_rows = dataset[dataset.isnull().any(axis=1)].head()\n",
    "sample_incomplete_rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing data. So we're all good. Now we will be diverge from pandas and use numpy to work with arrays in order to implement our Machine Learning Algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 1:32].values #Create an array of features where we drop the labels \n",
    "y = dataset.iloc[:, 0].values  #Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that we have a categorical feature in our dataset. Thus, we will now decide to encode it so our ML models can perform with any issues and errors. We will use one-hot encoding which is one of the most popular methods to encode categorical features, especially since it our categories are not ordinal. One hot encoding creates new (binary) columns, indicating the presence of each possible value from the original data. Let's work through this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 ... -0.8765252622477462 -1.303574775109286\n",
      "  -0.6858526551788927]\n",
      " [1.0 0.0 0.0 ... -0.2902906528139364 -0.4115378456929173\n",
      "  -2.506186182487661]\n",
      " [1.0 0.0 0.0 ... 0.2735273334152696 0.19125933115667776\n",
      "  -0.33644020270983244]\n",
      " ...\n",
      " [0.0 0.0 1.0 ... -0.24697382697675335 -0.0034428330797955787\n",
      "  0.7617113553888454]\n",
      " [0.0 0.0 1.0 ... -0.5985941843379471 1.9769820045028088\n",
      "  0.5074231017644218]\n",
      " [0.0 0.0 1.0 ... -0.32010412324815474 0.8177409675901581\n",
      "  -1.2747425297267416]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [30])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Categorical features are now encoded. \"A\" is now represented by the vector (1, 0, 0), \"B\" is now (0, 1, 0) and \"C\" is now  (0, 0, 1) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed to split our data into the training set and the test set, we need to ensure that there is no multicollinearity in our data. We shall use the df.corr() plot to show how the features might be correlated and thus possibily drop some columns contributing to multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>label</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009661</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>-0.027292</td>\n",
       "      <td>0.017680</td>\n",
       "      <td>0.360908</td>\n",
       "      <td>-0.094833</td>\n",
       "      <td>0.084037</td>\n",
       "      <td>-0.080787</td>\n",
       "      <td>-0.010207</td>\n",
       "      <td>-0.032810</td>\n",
       "      <td>0.026852</td>\n",
       "      <td>0.022983</td>\n",
       "      <td>0.160865</td>\n",
       "      <td>-0.017670</td>\n",
       "      <td>0.009823</td>\n",
       "      <td>0.024836</td>\n",
       "      <td>0.122280</td>\n",
       "      <td>-0.012269</td>\n",
       "      <td>-0.003363</td>\n",
       "      <td>-0.053276</td>\n",
       "      <td>-0.040523</td>\n",
       "      <td>-0.001074</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.018410</td>\n",
       "      <td>0.084037</td>\n",
       "      <td>-0.058086</td>\n",
       "      <td>0.030414</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.026621</td>\n",
       "      <td>-0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_1</td>\n",
       "      <td>0.009661</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082878</td>\n",
       "      <td>-0.038198</td>\n",
       "      <td>-0.471815</td>\n",
       "      <td>-0.085322</td>\n",
       "      <td>0.072784</td>\n",
       "      <td>0.038568</td>\n",
       "      <td>-0.158132</td>\n",
       "      <td>0.011850</td>\n",
       "      <td>-0.181452</td>\n",
       "      <td>0.040107</td>\n",
       "      <td>-0.072106</td>\n",
       "      <td>-0.067078</td>\n",
       "      <td>-0.045982</td>\n",
       "      <td>0.025476</td>\n",
       "      <td>0.029398</td>\n",
       "      <td>0.161614</td>\n",
       "      <td>0.024459</td>\n",
       "      <td>-0.016112</td>\n",
       "      <td>-0.018215</td>\n",
       "      <td>-0.037149</td>\n",
       "      <td>0.013212</td>\n",
       "      <td>0.037594</td>\n",
       "      <td>-0.022834</td>\n",
       "      <td>0.038568</td>\n",
       "      <td>0.492418</td>\n",
       "      <td>0.010559</td>\n",
       "      <td>0.027984</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>0.000516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_2</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.082878</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.029789</td>\n",
       "      <td>-0.021847</td>\n",
       "      <td>-0.016171</td>\n",
       "      <td>0.045615</td>\n",
       "      <td>-0.009897</td>\n",
       "      <td>-0.012176</td>\n",
       "      <td>0.034957</td>\n",
       "      <td>-0.000694</td>\n",
       "      <td>-0.001931</td>\n",
       "      <td>-0.022025</td>\n",
       "      <td>-0.007911</td>\n",
       "      <td>-0.027852</td>\n",
       "      <td>-0.023145</td>\n",
       "      <td>-0.020522</td>\n",
       "      <td>0.020464</td>\n",
       "      <td>0.016920</td>\n",
       "      <td>-0.071019</td>\n",
       "      <td>0.015169</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>-0.023536</td>\n",
       "      <td>0.053293</td>\n",
       "      <td>-0.009897</td>\n",
       "      <td>0.060571</td>\n",
       "      <td>0.005836</td>\n",
       "      <td>0.011006</td>\n",
       "      <td>-0.051986</td>\n",
       "      <td>-0.021119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_3</td>\n",
       "      <td>-0.027292</td>\n",
       "      <td>-0.038198</td>\n",
       "      <td>-0.029789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.038485</td>\n",
       "      <td>-0.039121</td>\n",
       "      <td>-0.038413</td>\n",
       "      <td>0.034297</td>\n",
       "      <td>-0.002464</td>\n",
       "      <td>-0.025788</td>\n",
       "      <td>-0.057455</td>\n",
       "      <td>0.013345</td>\n",
       "      <td>-0.017914</td>\n",
       "      <td>-0.026312</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>-0.025041</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>-0.029453</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.008337</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>-0.028417</td>\n",
       "      <td>-0.013685</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>-0.020110</td>\n",
       "      <td>0.034297</td>\n",
       "      <td>-0.054942</td>\n",
       "      <td>0.047069</td>\n",
       "      <td>0.012964</td>\n",
       "      <td>0.022155</td>\n",
       "      <td>0.004874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_4</td>\n",
       "      <td>0.017680</td>\n",
       "      <td>-0.471815</td>\n",
       "      <td>-0.021847</td>\n",
       "      <td>-0.038485</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.367359</td>\n",
       "      <td>0.365658</td>\n",
       "      <td>-0.552903</td>\n",
       "      <td>0.259633</td>\n",
       "      <td>0.039210</td>\n",
       "      <td>0.520221</td>\n",
       "      <td>-0.008051</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.325152</td>\n",
       "      <td>0.046668</td>\n",
       "      <td>0.029024</td>\n",
       "      <td>-0.038475</td>\n",
       "      <td>-0.494866</td>\n",
       "      <td>-0.004106</td>\n",
       "      <td>0.039993</td>\n",
       "      <td>-0.004260</td>\n",
       "      <td>0.040065</td>\n",
       "      <td>-0.038504</td>\n",
       "      <td>-0.008045</td>\n",
       "      <td>0.025285</td>\n",
       "      <td>-0.552903</td>\n",
       "      <td>-0.307397</td>\n",
       "      <td>0.011334</td>\n",
       "      <td>-0.087116</td>\n",
       "      <td>0.012474</td>\n",
       "      <td>-0.024360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_5</td>\n",
       "      <td>0.360908</td>\n",
       "      <td>-0.085322</td>\n",
       "      <td>-0.016171</td>\n",
       "      <td>-0.039121</td>\n",
       "      <td>0.367359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.131308</td>\n",
       "      <td>-0.323855</td>\n",
       "      <td>-0.127264</td>\n",
       "      <td>-0.009098</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>-0.033812</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>0.016373</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>-0.019051</td>\n",
       "      <td>-0.100867</td>\n",
       "      <td>-0.014685</td>\n",
       "      <td>0.032998</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>-0.007364</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.012849</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>-0.323855</td>\n",
       "      <td>-0.347500</td>\n",
       "      <td>0.018948</td>\n",
       "      <td>-0.039244</td>\n",
       "      <td>-0.001811</td>\n",
       "      <td>-0.009421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_6</td>\n",
       "      <td>-0.094833</td>\n",
       "      <td>0.072784</td>\n",
       "      <td>0.045615</td>\n",
       "      <td>-0.038413</td>\n",
       "      <td>0.365658</td>\n",
       "      <td>0.131308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.220093</td>\n",
       "      <td>-0.192308</td>\n",
       "      <td>0.056395</td>\n",
       "      <td>0.059554</td>\n",
       "      <td>-0.014067</td>\n",
       "      <td>-0.009265</td>\n",
       "      <td>-0.012837</td>\n",
       "      <td>0.011506</td>\n",
       "      <td>-0.022557</td>\n",
       "      <td>-0.030846</td>\n",
       "      <td>0.109483</td>\n",
       "      <td>-0.020908</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.022548</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.040181</td>\n",
       "      <td>-0.021773</td>\n",
       "      <td>0.002161</td>\n",
       "      <td>-0.220093</td>\n",
       "      <td>0.231713</td>\n",
       "      <td>-0.008333</td>\n",
       "      <td>-0.071596</td>\n",
       "      <td>-0.008043</td>\n",
       "      <td>0.004082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_7</td>\n",
       "      <td>0.084037</td>\n",
       "      <td>0.038568</td>\n",
       "      <td>-0.009897</td>\n",
       "      <td>0.034297</td>\n",
       "      <td>-0.552903</td>\n",
       "      <td>-0.323855</td>\n",
       "      <td>-0.220093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.331922</td>\n",
       "      <td>-0.023040</td>\n",
       "      <td>-0.557140</td>\n",
       "      <td>0.016133</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.511829</td>\n",
       "      <td>-0.018621</td>\n",
       "      <td>-0.058906</td>\n",
       "      <td>0.015564</td>\n",
       "      <td>0.429251</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>-0.038124</td>\n",
       "      <td>0.028060</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.027461</td>\n",
       "      <td>0.039540</td>\n",
       "      <td>0.009109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018245</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.015827</td>\n",
       "      <td>-0.020315</td>\n",
       "      <td>0.038426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_8</td>\n",
       "      <td>-0.080787</td>\n",
       "      <td>-0.158132</td>\n",
       "      <td>-0.012176</td>\n",
       "      <td>-0.002464</td>\n",
       "      <td>0.259633</td>\n",
       "      <td>-0.127264</td>\n",
       "      <td>-0.192308</td>\n",
       "      <td>-0.331922</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018428</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>-0.005395</td>\n",
       "      <td>0.006101</td>\n",
       "      <td>-0.024171</td>\n",
       "      <td>0.049476</td>\n",
       "      <td>0.056305</td>\n",
       "      <td>-0.003212</td>\n",
       "      <td>-0.030490</td>\n",
       "      <td>0.025249</td>\n",
       "      <td>0.020266</td>\n",
       "      <td>-0.038498</td>\n",
       "      <td>0.032268</td>\n",
       "      <td>-0.013269</td>\n",
       "      <td>-0.058599</td>\n",
       "      <td>-0.027558</td>\n",
       "      <td>-0.331922</td>\n",
       "      <td>0.241322</td>\n",
       "      <td>0.018349</td>\n",
       "      <td>-0.007180</td>\n",
       "      <td>-0.001855</td>\n",
       "      <td>-0.033252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_9</td>\n",
       "      <td>-0.010207</td>\n",
       "      <td>0.011850</td>\n",
       "      <td>0.034957</td>\n",
       "      <td>-0.025788</td>\n",
       "      <td>0.039210</td>\n",
       "      <td>-0.009098</td>\n",
       "      <td>0.056395</td>\n",
       "      <td>-0.023040</td>\n",
       "      <td>-0.018428</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009863</td>\n",
       "      <td>-0.014070</td>\n",
       "      <td>-0.009451</td>\n",
       "      <td>0.011529</td>\n",
       "      <td>0.039236</td>\n",
       "      <td>0.019593</td>\n",
       "      <td>-0.024093</td>\n",
       "      <td>-0.044001</td>\n",
       "      <td>-0.028579</td>\n",
       "      <td>-0.017970</td>\n",
       "      <td>-0.008911</td>\n",
       "      <td>-0.008751</td>\n",
       "      <td>-0.019719</td>\n",
       "      <td>0.044058</td>\n",
       "      <td>-0.050115</td>\n",
       "      <td>-0.023040</td>\n",
       "      <td>-0.013713</td>\n",
       "      <td>0.012709</td>\n",
       "      <td>0.029147</td>\n",
       "      <td>-0.035443</td>\n",
       "      <td>0.014737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_10</td>\n",
       "      <td>-0.032810</td>\n",
       "      <td>-0.181452</td>\n",
       "      <td>-0.000694</td>\n",
       "      <td>-0.057455</td>\n",
       "      <td>0.520221</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.059554</td>\n",
       "      <td>-0.557140</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>0.009863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031188</td>\n",
       "      <td>0.010876</td>\n",
       "      <td>-0.035251</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>0.060014</td>\n",
       "      <td>-0.028681</td>\n",
       "      <td>-0.082252</td>\n",
       "      <td>-0.013812</td>\n",
       "      <td>0.024857</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>0.024731</td>\n",
       "      <td>-0.006763</td>\n",
       "      <td>-0.006630</td>\n",
       "      <td>0.006139</td>\n",
       "      <td>-0.557140</td>\n",
       "      <td>0.209390</td>\n",
       "      <td>-0.014714</td>\n",
       "      <td>-0.027117</td>\n",
       "      <td>0.034707</td>\n",
       "      <td>-0.019796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_11</td>\n",
       "      <td>0.026852</td>\n",
       "      <td>0.040107</td>\n",
       "      <td>-0.001931</td>\n",
       "      <td>0.013345</td>\n",
       "      <td>-0.008051</td>\n",
       "      <td>-0.033812</td>\n",
       "      <td>-0.014067</td>\n",
       "      <td>0.016133</td>\n",
       "      <td>-0.005395</td>\n",
       "      <td>-0.014070</td>\n",
       "      <td>0.031188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005931</td>\n",
       "      <td>0.035918</td>\n",
       "      <td>-0.003921</td>\n",
       "      <td>-0.014533</td>\n",
       "      <td>-0.022671</td>\n",
       "      <td>0.011112</td>\n",
       "      <td>-0.003976</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>-0.001673</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>-0.012475</td>\n",
       "      <td>-0.091910</td>\n",
       "      <td>-0.024681</td>\n",
       "      <td>0.016133</td>\n",
       "      <td>0.037945</td>\n",
       "      <td>-0.031166</td>\n",
       "      <td>0.026032</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>-0.032396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_12</td>\n",
       "      <td>0.022983</td>\n",
       "      <td>-0.072106</td>\n",
       "      <td>-0.022025</td>\n",
       "      <td>-0.017914</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>-0.009265</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.006101</td>\n",
       "      <td>-0.009451</td>\n",
       "      <td>0.010876</td>\n",
       "      <td>0.005931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>-0.016759</td>\n",
       "      <td>0.040048</td>\n",
       "      <td>-0.010834</td>\n",
       "      <td>-0.005775</td>\n",
       "      <td>-0.014755</td>\n",
       "      <td>-0.026024</td>\n",
       "      <td>0.022893</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>-0.022879</td>\n",
       "      <td>-0.014520</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>-0.033940</td>\n",
       "      <td>0.005255</td>\n",
       "      <td>-0.017404</td>\n",
       "      <td>-0.012014</td>\n",
       "      <td>-0.067313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_13</td>\n",
       "      <td>0.160865</td>\n",
       "      <td>-0.067078</td>\n",
       "      <td>-0.007911</td>\n",
       "      <td>-0.026312</td>\n",
       "      <td>0.325152</td>\n",
       "      <td>0.016373</td>\n",
       "      <td>-0.012837</td>\n",
       "      <td>0.511829</td>\n",
       "      <td>-0.024171</td>\n",
       "      <td>0.011529</td>\n",
       "      <td>-0.035251</td>\n",
       "      <td>0.035918</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014028</td>\n",
       "      <td>-0.005176</td>\n",
       "      <td>-0.010868</td>\n",
       "      <td>-0.023626</td>\n",
       "      <td>0.015932</td>\n",
       "      <td>-0.000595</td>\n",
       "      <td>0.010766</td>\n",
       "      <td>0.036252</td>\n",
       "      <td>-0.068154</td>\n",
       "      <td>0.056863</td>\n",
       "      <td>0.027351</td>\n",
       "      <td>0.511829</td>\n",
       "      <td>-0.106188</td>\n",
       "      <td>0.049323</td>\n",
       "      <td>-0.063449</td>\n",
       "      <td>-0.003003</td>\n",
       "      <td>0.010566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_14</td>\n",
       "      <td>-0.017670</td>\n",
       "      <td>-0.045982</td>\n",
       "      <td>-0.027852</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>0.046668</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.011506</td>\n",
       "      <td>-0.018621</td>\n",
       "      <td>0.049476</td>\n",
       "      <td>0.039236</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>-0.003921</td>\n",
       "      <td>-0.016759</td>\n",
       "      <td>0.014028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024387</td>\n",
       "      <td>-0.022874</td>\n",
       "      <td>-0.014048</td>\n",
       "      <td>-0.000163</td>\n",
       "      <td>-0.012818</td>\n",
       "      <td>-0.027839</td>\n",
       "      <td>-0.064616</td>\n",
       "      <td>0.055829</td>\n",
       "      <td>-0.028865</td>\n",
       "      <td>-0.054727</td>\n",
       "      <td>-0.018621</td>\n",
       "      <td>-0.009918</td>\n",
       "      <td>-0.032146</td>\n",
       "      <td>-0.035036</td>\n",
       "      <td>-0.081380</td>\n",
       "      <td>-0.005302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_15</td>\n",
       "      <td>0.009823</td>\n",
       "      <td>0.025476</td>\n",
       "      <td>-0.023145</td>\n",
       "      <td>-0.025041</td>\n",
       "      <td>0.029024</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>-0.022557</td>\n",
       "      <td>-0.058906</td>\n",
       "      <td>0.056305</td>\n",
       "      <td>0.019593</td>\n",
       "      <td>0.060014</td>\n",
       "      <td>-0.014533</td>\n",
       "      <td>0.040048</td>\n",
       "      <td>-0.005176</td>\n",
       "      <td>-0.024387</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005312</td>\n",
       "      <td>-0.008005</td>\n",
       "      <td>0.093647</td>\n",
       "      <td>0.024842</td>\n",
       "      <td>0.036198</td>\n",
       "      <td>0.010157</td>\n",
       "      <td>-0.016786</td>\n",
       "      <td>0.014945</td>\n",
       "      <td>-0.014683</td>\n",
       "      <td>-0.058906</td>\n",
       "      <td>0.040498</td>\n",
       "      <td>-0.012235</td>\n",
       "      <td>-0.013391</td>\n",
       "      <td>-0.006121</td>\n",
       "      <td>-0.047371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_16</td>\n",
       "      <td>0.024836</td>\n",
       "      <td>0.029398</td>\n",
       "      <td>-0.020522</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>-0.038475</td>\n",
       "      <td>-0.019051</td>\n",
       "      <td>-0.030846</td>\n",
       "      <td>0.015564</td>\n",
       "      <td>-0.003212</td>\n",
       "      <td>-0.024093</td>\n",
       "      <td>-0.028681</td>\n",
       "      <td>-0.022671</td>\n",
       "      <td>-0.010834</td>\n",
       "      <td>-0.010868</td>\n",
       "      <td>-0.022874</td>\n",
       "      <td>0.005312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010795</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>0.041510</td>\n",
       "      <td>0.032061</td>\n",
       "      <td>-0.042459</td>\n",
       "      <td>0.033321</td>\n",
       "      <td>-0.032103</td>\n",
       "      <td>-0.062038</td>\n",
       "      <td>0.015564</td>\n",
       "      <td>-0.005607</td>\n",
       "      <td>0.030020</td>\n",
       "      <td>-0.025653</td>\n",
       "      <td>-0.008364</td>\n",
       "      <td>0.003649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_17</td>\n",
       "      <td>0.122280</td>\n",
       "      <td>0.161614</td>\n",
       "      <td>0.020464</td>\n",
       "      <td>-0.029453</td>\n",
       "      <td>-0.494866</td>\n",
       "      <td>-0.100867</td>\n",
       "      <td>0.109483</td>\n",
       "      <td>0.429251</td>\n",
       "      <td>-0.030490</td>\n",
       "      <td>-0.044001</td>\n",
       "      <td>-0.082252</td>\n",
       "      <td>0.011112</td>\n",
       "      <td>-0.005775</td>\n",
       "      <td>-0.023626</td>\n",
       "      <td>-0.014048</td>\n",
       "      <td>-0.008005</td>\n",
       "      <td>-0.010795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019030</td>\n",
       "      <td>-0.018384</td>\n",
       "      <td>0.023332</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.023643</td>\n",
       "      <td>-0.026874</td>\n",
       "      <td>-0.036680</td>\n",
       "      <td>0.429251</td>\n",
       "      <td>0.745246</td>\n",
       "      <td>0.007712</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>-0.010003</td>\n",
       "      <td>0.018319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_18</td>\n",
       "      <td>-0.012269</td>\n",
       "      <td>0.024459</td>\n",
       "      <td>0.016920</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>-0.004106</td>\n",
       "      <td>-0.014685</td>\n",
       "      <td>-0.020908</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>0.025249</td>\n",
       "      <td>-0.028579</td>\n",
       "      <td>-0.013812</td>\n",
       "      <td>-0.003976</td>\n",
       "      <td>-0.014755</td>\n",
       "      <td>0.015932</td>\n",
       "      <td>-0.000163</td>\n",
       "      <td>0.093647</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.019030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005534</td>\n",
       "      <td>0.035722</td>\n",
       "      <td>0.023711</td>\n",
       "      <td>-0.010677</td>\n",
       "      <td>-0.029830</td>\n",
       "      <td>-0.007277</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.012365</td>\n",
       "      <td>0.014546</td>\n",
       "      <td>-0.010325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_19</td>\n",
       "      <td>-0.003363</td>\n",
       "      <td>-0.016112</td>\n",
       "      <td>-0.071019</td>\n",
       "      <td>0.008337</td>\n",
       "      <td>0.039993</td>\n",
       "      <td>0.032998</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>-0.038124</td>\n",
       "      <td>0.020266</td>\n",
       "      <td>-0.017970</td>\n",
       "      <td>0.024857</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>-0.026024</td>\n",
       "      <td>-0.000595</td>\n",
       "      <td>-0.012818</td>\n",
       "      <td>0.024842</td>\n",
       "      <td>0.041510</td>\n",
       "      <td>-0.018384</td>\n",
       "      <td>-0.005534</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002679</td>\n",
       "      <td>0.012798</td>\n",
       "      <td>0.014513</td>\n",
       "      <td>-0.024631</td>\n",
       "      <td>0.067335</td>\n",
       "      <td>-0.038124</td>\n",
       "      <td>-0.011068</td>\n",
       "      <td>-0.034417</td>\n",
       "      <td>0.018706</td>\n",
       "      <td>0.039798</td>\n",
       "      <td>-0.036630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_20</td>\n",
       "      <td>-0.053276</td>\n",
       "      <td>-0.018215</td>\n",
       "      <td>0.015169</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>-0.004260</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.022548</td>\n",
       "      <td>0.028060</td>\n",
       "      <td>-0.038498</td>\n",
       "      <td>-0.008911</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>-0.001673</td>\n",
       "      <td>0.022893</td>\n",
       "      <td>0.010766</td>\n",
       "      <td>-0.027839</td>\n",
       "      <td>0.036198</td>\n",
       "      <td>0.032061</td>\n",
       "      <td>0.023332</td>\n",
       "      <td>0.035722</td>\n",
       "      <td>-0.002679</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012163</td>\n",
       "      <td>0.010581</td>\n",
       "      <td>-0.019166</td>\n",
       "      <td>-0.029720</td>\n",
       "      <td>0.028060</td>\n",
       "      <td>-0.004307</td>\n",
       "      <td>-0.004225</td>\n",
       "      <td>-0.034382</td>\n",
       "      <td>0.014969</td>\n",
       "      <td>0.013644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_21</td>\n",
       "      <td>-0.040523</td>\n",
       "      <td>-0.037149</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>-0.028417</td>\n",
       "      <td>0.040065</td>\n",
       "      <td>-0.007364</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.032268</td>\n",
       "      <td>-0.008751</td>\n",
       "      <td>0.024731</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.036252</td>\n",
       "      <td>-0.064616</td>\n",
       "      <td>0.010157</td>\n",
       "      <td>-0.042459</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.023711</td>\n",
       "      <td>0.012798</td>\n",
       "      <td>0.012163</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021333</td>\n",
       "      <td>-0.009808</td>\n",
       "      <td>-0.019683</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>0.022459</td>\n",
       "      <td>-0.019122</td>\n",
       "      <td>0.017053</td>\n",
       "      <td>-0.008550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_22</td>\n",
       "      <td>-0.001074</td>\n",
       "      <td>0.013212</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>-0.013685</td>\n",
       "      <td>-0.038504</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>-0.040181</td>\n",
       "      <td>-0.027461</td>\n",
       "      <td>-0.013269</td>\n",
       "      <td>-0.019719</td>\n",
       "      <td>-0.006763</td>\n",
       "      <td>-0.012475</td>\n",
       "      <td>-0.022879</td>\n",
       "      <td>-0.068154</td>\n",
       "      <td>0.055829</td>\n",
       "      <td>-0.016786</td>\n",
       "      <td>0.033321</td>\n",
       "      <td>-0.023643</td>\n",
       "      <td>-0.010677</td>\n",
       "      <td>0.014513</td>\n",
       "      <td>0.010581</td>\n",
       "      <td>-0.021333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003169</td>\n",
       "      <td>0.018023</td>\n",
       "      <td>-0.027461</td>\n",
       "      <td>-0.023277</td>\n",
       "      <td>-0.000476</td>\n",
       "      <td>-0.001558</td>\n",
       "      <td>0.023211</td>\n",
       "      <td>0.007716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_23</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.037594</td>\n",
       "      <td>-0.023536</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>-0.008045</td>\n",
       "      <td>0.012849</td>\n",
       "      <td>-0.021773</td>\n",
       "      <td>0.039540</td>\n",
       "      <td>-0.058599</td>\n",
       "      <td>0.044058</td>\n",
       "      <td>-0.006630</td>\n",
       "      <td>-0.091910</td>\n",
       "      <td>-0.014520</td>\n",
       "      <td>0.056863</td>\n",
       "      <td>-0.028865</td>\n",
       "      <td>0.014945</td>\n",
       "      <td>-0.032103</td>\n",
       "      <td>-0.026874</td>\n",
       "      <td>-0.029830</td>\n",
       "      <td>-0.024631</td>\n",
       "      <td>-0.019166</td>\n",
       "      <td>-0.009808</td>\n",
       "      <td>-0.003169</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035036</td>\n",
       "      <td>0.039540</td>\n",
       "      <td>-0.031544</td>\n",
       "      <td>0.019795</td>\n",
       "      <td>-0.036696</td>\n",
       "      <td>-0.013122</td>\n",
       "      <td>0.019533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_24</td>\n",
       "      <td>0.018410</td>\n",
       "      <td>-0.022834</td>\n",
       "      <td>0.053293</td>\n",
       "      <td>-0.020110</td>\n",
       "      <td>0.025285</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.009109</td>\n",
       "      <td>-0.027558</td>\n",
       "      <td>-0.050115</td>\n",
       "      <td>0.006139</td>\n",
       "      <td>-0.024681</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.027351</td>\n",
       "      <td>-0.054727</td>\n",
       "      <td>-0.014683</td>\n",
       "      <td>-0.062038</td>\n",
       "      <td>-0.036680</td>\n",
       "      <td>-0.007277</td>\n",
       "      <td>0.067335</td>\n",
       "      <td>-0.029720</td>\n",
       "      <td>-0.019683</td>\n",
       "      <td>0.018023</td>\n",
       "      <td>0.035036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009109</td>\n",
       "      <td>-0.043074</td>\n",
       "      <td>-0.004420</td>\n",
       "      <td>-0.061695</td>\n",
       "      <td>0.048232</td>\n",
       "      <td>0.021550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_25</td>\n",
       "      <td>0.084037</td>\n",
       "      <td>0.038568</td>\n",
       "      <td>-0.009897</td>\n",
       "      <td>0.034297</td>\n",
       "      <td>-0.552903</td>\n",
       "      <td>-0.323855</td>\n",
       "      <td>-0.220093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.331922</td>\n",
       "      <td>-0.023040</td>\n",
       "      <td>-0.557140</td>\n",
       "      <td>0.016133</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.511829</td>\n",
       "      <td>-0.018621</td>\n",
       "      <td>-0.058906</td>\n",
       "      <td>0.015564</td>\n",
       "      <td>0.429251</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>-0.038124</td>\n",
       "      <td>0.028060</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.027461</td>\n",
       "      <td>0.039540</td>\n",
       "      <td>0.009109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018245</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.015827</td>\n",
       "      <td>-0.020315</td>\n",
       "      <td>0.038426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_26</td>\n",
       "      <td>-0.058086</td>\n",
       "      <td>0.492418</td>\n",
       "      <td>0.060571</td>\n",
       "      <td>-0.054942</td>\n",
       "      <td>-0.307397</td>\n",
       "      <td>-0.347500</td>\n",
       "      <td>0.231713</td>\n",
       "      <td>0.018245</td>\n",
       "      <td>0.241322</td>\n",
       "      <td>-0.013713</td>\n",
       "      <td>0.209390</td>\n",
       "      <td>0.037945</td>\n",
       "      <td>-0.033940</td>\n",
       "      <td>-0.106188</td>\n",
       "      <td>-0.009918</td>\n",
       "      <td>0.040498</td>\n",
       "      <td>-0.005607</td>\n",
       "      <td>0.745246</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>-0.011068</td>\n",
       "      <td>-0.004307</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>-0.023277</td>\n",
       "      <td>-0.031544</td>\n",
       "      <td>-0.043074</td>\n",
       "      <td>0.018245</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003771</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>-0.002798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_27</td>\n",
       "      <td>0.030414</td>\n",
       "      <td>0.010559</td>\n",
       "      <td>0.005836</td>\n",
       "      <td>0.047069</td>\n",
       "      <td>0.011334</td>\n",
       "      <td>0.018948</td>\n",
       "      <td>-0.008333</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.018349</td>\n",
       "      <td>0.012709</td>\n",
       "      <td>-0.014714</td>\n",
       "      <td>-0.031166</td>\n",
       "      <td>0.005255</td>\n",
       "      <td>0.049323</td>\n",
       "      <td>-0.032146</td>\n",
       "      <td>-0.012235</td>\n",
       "      <td>0.030020</td>\n",
       "      <td>0.007712</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.034417</td>\n",
       "      <td>-0.004225</td>\n",
       "      <td>0.022459</td>\n",
       "      <td>-0.000476</td>\n",
       "      <td>0.019795</td>\n",
       "      <td>-0.004420</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.003771</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.043569</td>\n",
       "      <td>0.029012</td>\n",
       "      <td>0.038345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_28</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.027984</td>\n",
       "      <td>0.011006</td>\n",
       "      <td>0.012964</td>\n",
       "      <td>-0.087116</td>\n",
       "      <td>-0.039244</td>\n",
       "      <td>-0.071596</td>\n",
       "      <td>0.015827</td>\n",
       "      <td>-0.007180</td>\n",
       "      <td>0.029147</td>\n",
       "      <td>-0.027117</td>\n",
       "      <td>0.026032</td>\n",
       "      <td>-0.017404</td>\n",
       "      <td>-0.063449</td>\n",
       "      <td>-0.035036</td>\n",
       "      <td>-0.013391</td>\n",
       "      <td>-0.025653</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>-0.012365</td>\n",
       "      <td>0.018706</td>\n",
       "      <td>-0.034382</td>\n",
       "      <td>-0.019122</td>\n",
       "      <td>-0.001558</td>\n",
       "      <td>-0.036696</td>\n",
       "      <td>-0.061695</td>\n",
       "      <td>0.015827</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>-0.043569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043778</td>\n",
       "      <td>-0.052627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_29</td>\n",
       "      <td>0.026621</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>-0.051986</td>\n",
       "      <td>0.022155</td>\n",
       "      <td>0.012474</td>\n",
       "      <td>-0.001811</td>\n",
       "      <td>-0.008043</td>\n",
       "      <td>-0.020315</td>\n",
       "      <td>-0.001855</td>\n",
       "      <td>-0.035443</td>\n",
       "      <td>0.034707</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>-0.012014</td>\n",
       "      <td>-0.003003</td>\n",
       "      <td>-0.081380</td>\n",
       "      <td>-0.006121</td>\n",
       "      <td>-0.008364</td>\n",
       "      <td>-0.010003</td>\n",
       "      <td>0.014546</td>\n",
       "      <td>0.039798</td>\n",
       "      <td>0.014969</td>\n",
       "      <td>0.017053</td>\n",
       "      <td>0.023211</td>\n",
       "      <td>-0.013122</td>\n",
       "      <td>0.048232</td>\n",
       "      <td>-0.020315</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.029012</td>\n",
       "      <td>0.043778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.031454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>feature_30</td>\n",
       "      <td>-0.003906</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>-0.021119</td>\n",
       "      <td>0.004874</td>\n",
       "      <td>-0.024360</td>\n",
       "      <td>-0.009421</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.038426</td>\n",
       "      <td>-0.033252</td>\n",
       "      <td>0.014737</td>\n",
       "      <td>-0.019796</td>\n",
       "      <td>-0.032396</td>\n",
       "      <td>-0.067313</td>\n",
       "      <td>0.010566</td>\n",
       "      <td>-0.005302</td>\n",
       "      <td>-0.047371</td>\n",
       "      <td>0.003649</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>-0.010325</td>\n",
       "      <td>-0.036630</td>\n",
       "      <td>0.013644</td>\n",
       "      <td>-0.008550</td>\n",
       "      <td>0.007716</td>\n",
       "      <td>0.019533</td>\n",
       "      <td>0.021550</td>\n",
       "      <td>0.038426</td>\n",
       "      <td>-0.002798</td>\n",
       "      <td>0.038345</td>\n",
       "      <td>-0.052627</td>\n",
       "      <td>-0.031454</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "label       1.000000   0.009661   0.000082  -0.027292   0.017680   0.360908   \n",
       "feature_1   0.009661   1.000000   0.082878  -0.038198  -0.471815  -0.085322   \n",
       "feature_2   0.000082   0.082878   1.000000  -0.029789  -0.021847  -0.016171   \n",
       "feature_3  -0.027292  -0.038198  -0.029789   1.000000  -0.038485  -0.039121   \n",
       "feature_4   0.017680  -0.471815  -0.021847  -0.038485   1.000000   0.367359   \n",
       "feature_5   0.360908  -0.085322  -0.016171  -0.039121   0.367359   1.000000   \n",
       "feature_6  -0.094833   0.072784   0.045615  -0.038413   0.365658   0.131308   \n",
       "feature_7   0.084037   0.038568  -0.009897   0.034297  -0.552903  -0.323855   \n",
       "feature_8  -0.080787  -0.158132  -0.012176  -0.002464   0.259633  -0.127264   \n",
       "feature_9  -0.010207   0.011850   0.034957  -0.025788   0.039210  -0.009098   \n",
       "feature_10 -0.032810  -0.181452  -0.000694  -0.057455   0.520221   0.002568   \n",
       "feature_11  0.026852   0.040107  -0.001931   0.013345  -0.008051  -0.033812   \n",
       "feature_12  0.022983  -0.072106  -0.022025  -0.017914   0.024800   0.002332   \n",
       "feature_13  0.160865  -0.067078  -0.007911  -0.026312   0.325152   0.016373   \n",
       "feature_14 -0.017670  -0.045982  -0.027852   0.033561   0.046668   0.000696   \n",
       "feature_15  0.009823   0.025476  -0.023145  -0.025041   0.029024   0.001927   \n",
       "feature_16  0.024836   0.029398  -0.020522   0.002794  -0.038475  -0.019051   \n",
       "feature_17  0.122280   0.161614   0.020464  -0.029453  -0.494866  -0.100867   \n",
       "feature_18 -0.012269   0.024459   0.016920   0.046700  -0.004106  -0.014685   \n",
       "feature_19 -0.003363  -0.016112  -0.071019   0.008337   0.039993   0.032998   \n",
       "feature_20 -0.053276  -0.018215   0.015169   0.015141  -0.004260   0.005518   \n",
       "feature_21 -0.040523  -0.037149   0.006483  -0.028417   0.040065  -0.007364   \n",
       "feature_22 -0.001074   0.013212   0.005106  -0.013685  -0.038504   0.005609   \n",
       "feature_23  0.000528   0.037594  -0.023536   0.001060  -0.008045   0.012849   \n",
       "feature_24  0.018410  -0.022834   0.053293  -0.020110   0.025285   0.001947   \n",
       "feature_25  0.084037   0.038568  -0.009897   0.034297  -0.552903  -0.323855   \n",
       "feature_26 -0.058086   0.492418   0.060571  -0.054942  -0.307397  -0.347500   \n",
       "feature_27  0.030414   0.010559   0.005836   0.047069   0.011334   0.018948   \n",
       "feature_28  0.009650   0.027984   0.011006   0.012964  -0.087116  -0.039244   \n",
       "feature_29  0.026621   0.002332  -0.051986   0.022155   0.012474  -0.001811   \n",
       "feature_30 -0.003906   0.000516  -0.021119   0.004874  -0.024360  -0.009421   \n",
       "\n",
       "            feature_6  feature_7  feature_8  feature_9  feature_10  \\\n",
       "label       -0.094833   0.084037  -0.080787  -0.010207   -0.032810   \n",
       "feature_1    0.072784   0.038568  -0.158132   0.011850   -0.181452   \n",
       "feature_2    0.045615  -0.009897  -0.012176   0.034957   -0.000694   \n",
       "feature_3   -0.038413   0.034297  -0.002464  -0.025788   -0.057455   \n",
       "feature_4    0.365658  -0.552903   0.259633   0.039210    0.520221   \n",
       "feature_5    0.131308  -0.323855  -0.127264  -0.009098    0.002568   \n",
       "feature_6    1.000000  -0.220093  -0.192308   0.056395    0.059554   \n",
       "feature_7   -0.220093   1.000000  -0.331922  -0.023040   -0.557140   \n",
       "feature_8   -0.192308  -0.331922   1.000000  -0.018428    0.017708   \n",
       "feature_9    0.056395  -0.023040  -0.018428   1.000000    0.009863   \n",
       "feature_10   0.059554  -0.557140   0.017708   0.009863    1.000000   \n",
       "feature_11  -0.014067   0.016133  -0.005395  -0.014070    0.031188   \n",
       "feature_12  -0.009265   0.003251   0.006101  -0.009451    0.010876   \n",
       "feature_13  -0.012837   0.511829  -0.024171   0.011529   -0.035251   \n",
       "feature_14   0.011506  -0.018621   0.049476   0.039236    0.006884   \n",
       "feature_15  -0.022557  -0.058906   0.056305   0.019593    0.060014   \n",
       "feature_16  -0.030846   0.015564  -0.003212  -0.024093   -0.028681   \n",
       "feature_17   0.109483   0.429251  -0.030490  -0.044001   -0.082252   \n",
       "feature_18  -0.020908   0.002657   0.025249  -0.028579   -0.013812   \n",
       "feature_19   0.004640  -0.038124   0.020266  -0.017970    0.024857   \n",
       "feature_20   0.022548   0.028060  -0.038498  -0.008911   -0.003766   \n",
       "feature_21  -0.000027   0.000006   0.032268  -0.008751    0.024731   \n",
       "feature_22  -0.040181  -0.027461  -0.013269  -0.019719   -0.006763   \n",
       "feature_23  -0.021773   0.039540  -0.058599   0.044058   -0.006630   \n",
       "feature_24   0.002161   0.009109  -0.027558  -0.050115    0.006139   \n",
       "feature_25  -0.220093   1.000000  -0.331922  -0.023040   -0.557140   \n",
       "feature_26   0.231713   0.018245   0.241322  -0.013713    0.209390   \n",
       "feature_27  -0.008333   0.021266   0.018349   0.012709   -0.014714   \n",
       "feature_28  -0.071596   0.015827  -0.007180   0.029147   -0.027117   \n",
       "feature_29  -0.008043  -0.020315  -0.001855  -0.035443    0.034707   \n",
       "feature_30   0.004082   0.038426  -0.033252   0.014737   -0.019796   \n",
       "\n",
       "            feature_11  feature_12  feature_13  feature_14  feature_15  \\\n",
       "label         0.026852    0.022983    0.160865   -0.017670    0.009823   \n",
       "feature_1     0.040107   -0.072106   -0.067078   -0.045982    0.025476   \n",
       "feature_2    -0.001931   -0.022025   -0.007911   -0.027852   -0.023145   \n",
       "feature_3     0.013345   -0.017914   -0.026312    0.033561   -0.025041   \n",
       "feature_4    -0.008051    0.024800    0.325152    0.046668    0.029024   \n",
       "feature_5    -0.033812    0.002332    0.016373    0.000696    0.001927   \n",
       "feature_6    -0.014067   -0.009265   -0.012837    0.011506   -0.022557   \n",
       "feature_7     0.016133    0.003251    0.511829   -0.018621   -0.058906   \n",
       "feature_8    -0.005395    0.006101   -0.024171    0.049476    0.056305   \n",
       "feature_9    -0.014070   -0.009451    0.011529    0.039236    0.019593   \n",
       "feature_10    0.031188    0.010876   -0.035251    0.006884    0.060014   \n",
       "feature_11    1.000000    0.005931    0.035918   -0.003921   -0.014533   \n",
       "feature_12    0.005931    1.000000    0.000832   -0.016759    0.040048   \n",
       "feature_13    0.035918    0.000832    1.000000    0.014028   -0.005176   \n",
       "feature_14   -0.003921   -0.016759    0.014028    1.000000   -0.024387   \n",
       "feature_15   -0.014533    0.040048   -0.005176   -0.024387    1.000000   \n",
       "feature_16   -0.022671   -0.010834   -0.010868   -0.022874    0.005312   \n",
       "feature_17    0.011112   -0.005775   -0.023626   -0.014048   -0.008005   \n",
       "feature_18   -0.003976   -0.014755    0.015932   -0.000163    0.093647   \n",
       "feature_19    0.007576   -0.026024   -0.000595   -0.012818    0.024842   \n",
       "feature_20   -0.001673    0.022893    0.010766   -0.027839    0.036198   \n",
       "feature_21    0.003346    0.001018    0.036252   -0.064616    0.010157   \n",
       "feature_22   -0.012475   -0.022879   -0.068154    0.055829   -0.016786   \n",
       "feature_23   -0.091910   -0.014520    0.056863   -0.028865    0.014945   \n",
       "feature_24   -0.024681    0.000813    0.027351   -0.054727   -0.014683   \n",
       "feature_25    0.016133    0.003251    0.511829   -0.018621   -0.058906   \n",
       "feature_26    0.037945   -0.033940   -0.106188   -0.009918    0.040498   \n",
       "feature_27   -0.031166    0.005255    0.049323   -0.032146   -0.012235   \n",
       "feature_28    0.026032   -0.017404   -0.063449   -0.035036   -0.013391   \n",
       "feature_29    0.001623   -0.012014   -0.003003   -0.081380   -0.006121   \n",
       "feature_30   -0.032396   -0.067313    0.010566   -0.005302   -0.047371   \n",
       "\n",
       "            feature_16  feature_17  feature_18  feature_19  feature_20  \\\n",
       "label         0.024836    0.122280   -0.012269   -0.003363   -0.053276   \n",
       "feature_1     0.029398    0.161614    0.024459   -0.016112   -0.018215   \n",
       "feature_2    -0.020522    0.020464    0.016920   -0.071019    0.015169   \n",
       "feature_3     0.002794   -0.029453    0.046700    0.008337    0.015141   \n",
       "feature_4    -0.038475   -0.494866   -0.004106    0.039993   -0.004260   \n",
       "feature_5    -0.019051   -0.100867   -0.014685    0.032998    0.005518   \n",
       "feature_6    -0.030846    0.109483   -0.020908    0.004640    0.022548   \n",
       "feature_7     0.015564    0.429251    0.002657   -0.038124    0.028060   \n",
       "feature_8    -0.003212   -0.030490    0.025249    0.020266   -0.038498   \n",
       "feature_9    -0.024093   -0.044001   -0.028579   -0.017970   -0.008911   \n",
       "feature_10   -0.028681   -0.082252   -0.013812    0.024857   -0.003766   \n",
       "feature_11   -0.022671    0.011112   -0.003976    0.007576   -0.001673   \n",
       "feature_12   -0.010834   -0.005775   -0.014755   -0.026024    0.022893   \n",
       "feature_13   -0.010868   -0.023626    0.015932   -0.000595    0.010766   \n",
       "feature_14   -0.022874   -0.014048   -0.000163   -0.012818   -0.027839   \n",
       "feature_15    0.005312   -0.008005    0.093647    0.024842    0.036198   \n",
       "feature_16    1.000000   -0.010795   -0.000099    0.041510    0.032061   \n",
       "feature_17   -0.010795    1.000000   -0.019030   -0.018384    0.023332   \n",
       "feature_18   -0.000099   -0.019030    1.000000   -0.005534    0.035722   \n",
       "feature_19    0.041510   -0.018384   -0.005534    1.000000   -0.002679   \n",
       "feature_20    0.032061    0.023332    0.035722   -0.002679    1.000000   \n",
       "feature_21   -0.042459    0.000177    0.023711    0.012798    0.012163   \n",
       "feature_22    0.033321   -0.023643   -0.010677    0.014513    0.010581   \n",
       "feature_23   -0.032103   -0.026874   -0.029830   -0.024631   -0.019166   \n",
       "feature_24   -0.062038   -0.036680   -0.007277    0.067335   -0.029720   \n",
       "feature_25    0.015564    0.429251    0.002657   -0.038124    0.028060   \n",
       "feature_26   -0.005607    0.745246    0.001275   -0.011068   -0.004307   \n",
       "feature_27    0.030020    0.007712    0.000027   -0.034417   -0.004225   \n",
       "feature_28   -0.025653    0.008418   -0.012365    0.018706   -0.034382   \n",
       "feature_29   -0.008364   -0.010003    0.014546    0.039798    0.014969   \n",
       "feature_30    0.003649    0.018319   -0.010325   -0.036630    0.013644   \n",
       "\n",
       "            feature_21  feature_22  feature_23  feature_24  feature_25  \\\n",
       "label        -0.040523   -0.001074    0.000528    0.018410    0.084037   \n",
       "feature_1    -0.037149    0.013212    0.037594   -0.022834    0.038568   \n",
       "feature_2     0.006483    0.005106   -0.023536    0.053293   -0.009897   \n",
       "feature_3    -0.028417   -0.013685    0.001060   -0.020110    0.034297   \n",
       "feature_4     0.040065   -0.038504   -0.008045    0.025285   -0.552903   \n",
       "feature_5    -0.007364    0.005609    0.012849    0.001947   -0.323855   \n",
       "feature_6    -0.000027   -0.040181   -0.021773    0.002161   -0.220093   \n",
       "feature_7     0.000006   -0.027461    0.039540    0.009109    1.000000   \n",
       "feature_8     0.032268   -0.013269   -0.058599   -0.027558   -0.331922   \n",
       "feature_9    -0.008751   -0.019719    0.044058   -0.050115   -0.023040   \n",
       "feature_10    0.024731   -0.006763   -0.006630    0.006139   -0.557140   \n",
       "feature_11    0.003346   -0.012475   -0.091910   -0.024681    0.016133   \n",
       "feature_12    0.001018   -0.022879   -0.014520    0.000813    0.003251   \n",
       "feature_13    0.036252   -0.068154    0.056863    0.027351    0.511829   \n",
       "feature_14   -0.064616    0.055829   -0.028865   -0.054727   -0.018621   \n",
       "feature_15    0.010157   -0.016786    0.014945   -0.014683   -0.058906   \n",
       "feature_16   -0.042459    0.033321   -0.032103   -0.062038    0.015564   \n",
       "feature_17    0.000177   -0.023643   -0.026874   -0.036680    0.429251   \n",
       "feature_18    0.023711   -0.010677   -0.029830   -0.007277    0.002657   \n",
       "feature_19    0.012798    0.014513   -0.024631    0.067335   -0.038124   \n",
       "feature_20    0.012163    0.010581   -0.019166   -0.029720    0.028060   \n",
       "feature_21    1.000000   -0.021333   -0.009808   -0.019683    0.000006   \n",
       "feature_22   -0.021333    1.000000   -0.003169    0.018023   -0.027461   \n",
       "feature_23   -0.009808   -0.003169    1.000000    0.035036    0.039540   \n",
       "feature_24   -0.019683    0.018023    0.035036    1.000000    0.009109   \n",
       "feature_25    0.000006   -0.027461    0.039540    0.009109    1.000000   \n",
       "feature_26    0.002417   -0.023277   -0.031544   -0.043074    0.018245   \n",
       "feature_27    0.022459   -0.000476    0.019795   -0.004420    0.021266   \n",
       "feature_28   -0.019122   -0.001558   -0.036696   -0.061695    0.015827   \n",
       "feature_29    0.017053    0.023211   -0.013122    0.048232   -0.020315   \n",
       "feature_30   -0.008550    0.007716    0.019533    0.021550    0.038426   \n",
       "\n",
       "            feature_26  feature_27  feature_28  feature_29  feature_30  \n",
       "label        -0.058086    0.030414    0.009650    0.026621   -0.003906  \n",
       "feature_1     0.492418    0.010559    0.027984    0.002332    0.000516  \n",
       "feature_2     0.060571    0.005836    0.011006   -0.051986   -0.021119  \n",
       "feature_3    -0.054942    0.047069    0.012964    0.022155    0.004874  \n",
       "feature_4    -0.307397    0.011334   -0.087116    0.012474   -0.024360  \n",
       "feature_5    -0.347500    0.018948   -0.039244   -0.001811   -0.009421  \n",
       "feature_6     0.231713   -0.008333   -0.071596   -0.008043    0.004082  \n",
       "feature_7     0.018245    0.021266    0.015827   -0.020315    0.038426  \n",
       "feature_8     0.241322    0.018349   -0.007180   -0.001855   -0.033252  \n",
       "feature_9    -0.013713    0.012709    0.029147   -0.035443    0.014737  \n",
       "feature_10    0.209390   -0.014714   -0.027117    0.034707   -0.019796  \n",
       "feature_11    0.037945   -0.031166    0.026032    0.001623   -0.032396  \n",
       "feature_12   -0.033940    0.005255   -0.017404   -0.012014   -0.067313  \n",
       "feature_13   -0.106188    0.049323   -0.063449   -0.003003    0.010566  \n",
       "feature_14   -0.009918   -0.032146   -0.035036   -0.081380   -0.005302  \n",
       "feature_15    0.040498   -0.012235   -0.013391   -0.006121   -0.047371  \n",
       "feature_16   -0.005607    0.030020   -0.025653   -0.008364    0.003649  \n",
       "feature_17    0.745246    0.007712    0.008418   -0.010003    0.018319  \n",
       "feature_18    0.001275    0.000027   -0.012365    0.014546   -0.010325  \n",
       "feature_19   -0.011068   -0.034417    0.018706    0.039798   -0.036630  \n",
       "feature_20   -0.004307   -0.004225   -0.034382    0.014969    0.013644  \n",
       "feature_21    0.002417    0.022459   -0.019122    0.017053   -0.008550  \n",
       "feature_22   -0.023277   -0.000476   -0.001558    0.023211    0.007716  \n",
       "feature_23   -0.031544    0.019795   -0.036696   -0.013122    0.019533  \n",
       "feature_24   -0.043074   -0.004420   -0.061695    0.048232    0.021550  \n",
       "feature_25    0.018245    0.021266    0.015827   -0.020315    0.038426  \n",
       "feature_26    1.000000    0.003771    0.003794    0.004154   -0.002798  \n",
       "feature_27    0.003771    1.000000   -0.043569    0.029012    0.038345  \n",
       "feature_28    0.003794   -0.043569    1.000000    0.043778   -0.052627  \n",
       "feature_29    0.004154    0.029012    0.043778    1.000000   -0.031454  \n",
       "feature_30   -0.002798    0.038345   -0.052627   -0.031454    1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations = dataset.corr()\n",
    "pd.set_option('display.max_columns', None)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the table, features 7 and 25 are perfectly correlated. This can result in multicollinearity. As a result, we will drop one of those columns to ensure performance gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>categorical_feature_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.623405</td>\n",
       "      <td>1.157472</td>\n",
       "      <td>0.978446</td>\n",
       "      <td>3.951788</td>\n",
       "      <td>0.918710</td>\n",
       "      <td>-0.742102</td>\n",
       "      <td>-1.764838</td>\n",
       "      <td>1.959709</td>\n",
       "      <td>-0.430550</td>\n",
       "      <td>1.712772</td>\n",
       "      <td>-0.859864</td>\n",
       "      <td>0.858919</td>\n",
       "      <td>1.633362</td>\n",
       "      <td>0.185770</td>\n",
       "      <td>1.232641</td>\n",
       "      <td>-1.126940</td>\n",
       "      <td>-0.559032</td>\n",
       "      <td>1.944720</td>\n",
       "      <td>-0.758760</td>\n",
       "      <td>0.816431</td>\n",
       "      <td>1.126701</td>\n",
       "      <td>1.245197</td>\n",
       "      <td>-0.349880</td>\n",
       "      <td>1.193921</td>\n",
       "      <td>-0.030706</td>\n",
       "      <td>-1.376937</td>\n",
       "      <td>-0.876525</td>\n",
       "      <td>-1.303575</td>\n",
       "      <td>-0.685853</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.017604</td>\n",
       "      <td>0.962947</td>\n",
       "      <td>1.971217</td>\n",
       "      <td>-2.183554</td>\n",
       "      <td>-2.475973</td>\n",
       "      <td>-4.056342</td>\n",
       "      <td>0.148193</td>\n",
       "      <td>0.529200</td>\n",
       "      <td>-0.692605</td>\n",
       "      <td>-0.036681</td>\n",
       "      <td>0.541161</td>\n",
       "      <td>-0.094085</td>\n",
       "      <td>-2.196636</td>\n",
       "      <td>0.500172</td>\n",
       "      <td>-0.075832</td>\n",
       "      <td>-1.428413</td>\n",
       "      <td>-2.433943</td>\n",
       "      <td>0.930740</td>\n",
       "      <td>0.374944</td>\n",
       "      <td>-1.863622</td>\n",
       "      <td>1.710424</td>\n",
       "      <td>-0.266497</td>\n",
       "      <td>1.327070</td>\n",
       "      <td>1.646004</td>\n",
       "      <td>-4.494677</td>\n",
       "      <td>-0.890755</td>\n",
       "      <td>-0.290291</td>\n",
       "      <td>-0.411538</td>\n",
       "      <td>-2.506186</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.814985</td>\n",
       "      <td>-2.619374</td>\n",
       "      <td>-0.777541</td>\n",
       "      <td>1.921058</td>\n",
       "      <td>-0.550762</td>\n",
       "      <td>1.097364</td>\n",
       "      <td>0.453402</td>\n",
       "      <td>0.426274</td>\n",
       "      <td>1.081457</td>\n",
       "      <td>1.688545</td>\n",
       "      <td>0.469907</td>\n",
       "      <td>-0.556450</td>\n",
       "      <td>0.511860</td>\n",
       "      <td>-0.820182</td>\n",
       "      <td>0.165401</td>\n",
       "      <td>0.971337</td>\n",
       "      <td>1.467646</td>\n",
       "      <td>-1.848128</td>\n",
       "      <td>1.346273</td>\n",
       "      <td>-0.846622</td>\n",
       "      <td>0.296170</td>\n",
       "      <td>-1.147455</td>\n",
       "      <td>-0.289827</td>\n",
       "      <td>0.043222</td>\n",
       "      <td>1.512044</td>\n",
       "      <td>0.193947</td>\n",
       "      <td>0.273527</td>\n",
       "      <td>0.191259</td>\n",
       "      <td>-0.336440</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.253860</td>\n",
       "      <td>0.276528</td>\n",
       "      <td>1.124507</td>\n",
       "      <td>-0.522192</td>\n",
       "      <td>1.223016</td>\n",
       "      <td>-0.521033</td>\n",
       "      <td>1.267149</td>\n",
       "      <td>0.400720</td>\n",
       "      <td>-0.480437</td>\n",
       "      <td>-0.540828</td>\n",
       "      <td>-1.385455</td>\n",
       "      <td>-0.310562</td>\n",
       "      <td>0.668803</td>\n",
       "      <td>-1.082398</td>\n",
       "      <td>-1.223218</td>\n",
       "      <td>0.187642</td>\n",
       "      <td>1.001025</td>\n",
       "      <td>-0.867119</td>\n",
       "      <td>-0.313673</td>\n",
       "      <td>0.596596</td>\n",
       "      <td>-0.592904</td>\n",
       "      <td>-0.490817</td>\n",
       "      <td>0.365089</td>\n",
       "      <td>-0.612431</td>\n",
       "      <td>0.385040</td>\n",
       "      <td>-0.317562</td>\n",
       "      <td>0.570157</td>\n",
       "      <td>0.224122</td>\n",
       "      <td>0.808667</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.119872</td>\n",
       "      <td>0.293963</td>\n",
       "      <td>0.421747</td>\n",
       "      <td>4.960528</td>\n",
       "      <td>-1.371113</td>\n",
       "      <td>-1.611126</td>\n",
       "      <td>-0.271988</td>\n",
       "      <td>3.291267</td>\n",
       "      <td>0.329875</td>\n",
       "      <td>1.025807</td>\n",
       "      <td>0.464537</td>\n",
       "      <td>1.570395</td>\n",
       "      <td>3.063703</td>\n",
       "      <td>0.451820</td>\n",
       "      <td>1.708836</td>\n",
       "      <td>-1.790733</td>\n",
       "      <td>-2.399010</td>\n",
       "      <td>-1.059323</td>\n",
       "      <td>-0.059301</td>\n",
       "      <td>-1.674795</td>\n",
       "      <td>0.511317</td>\n",
       "      <td>1.838477</td>\n",
       "      <td>1.946403</td>\n",
       "      <td>0.495256</td>\n",
       "      <td>-1.412096</td>\n",
       "      <td>0.891830</td>\n",
       "      <td>1.858374</td>\n",
       "      <td>0.998611</td>\n",
       "      <td>0.720807</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1295</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.791242</td>\n",
       "      <td>0.947756</td>\n",
       "      <td>0.332781</td>\n",
       "      <td>2.080631</td>\n",
       "      <td>-1.414817</td>\n",
       "      <td>2.668325</td>\n",
       "      <td>1.109606</td>\n",
       "      <td>-0.559478</td>\n",
       "      <td>0.891290</td>\n",
       "      <td>0.964982</td>\n",
       "      <td>-1.742831</td>\n",
       "      <td>-0.096719</td>\n",
       "      <td>0.975724</td>\n",
       "      <td>1.335518</td>\n",
       "      <td>-0.061370</td>\n",
       "      <td>0.162321</td>\n",
       "      <td>0.599685</td>\n",
       "      <td>1.782970</td>\n",
       "      <td>-0.172168</td>\n",
       "      <td>-1.074772</td>\n",
       "      <td>0.589416</td>\n",
       "      <td>0.874111</td>\n",
       "      <td>0.341586</td>\n",
       "      <td>-1.272205</td>\n",
       "      <td>1.240317</td>\n",
       "      <td>-0.210570</td>\n",
       "      <td>0.393873</td>\n",
       "      <td>0.936976</td>\n",
       "      <td>0.782123</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1296</td>\n",
       "      <td>0</td>\n",
       "      <td>0.251327</td>\n",
       "      <td>0.304282</td>\n",
       "      <td>-0.490810</td>\n",
       "      <td>0.704915</td>\n",
       "      <td>-0.440763</td>\n",
       "      <td>3.088168</td>\n",
       "      <td>-2.379913</td>\n",
       "      <td>-0.172848</td>\n",
       "      <td>1.768220</td>\n",
       "      <td>1.124578</td>\n",
       "      <td>1.252319</td>\n",
       "      <td>0.792092</td>\n",
       "      <td>-1.533918</td>\n",
       "      <td>-1.778728</td>\n",
       "      <td>0.485011</td>\n",
       "      <td>-0.825671</td>\n",
       "      <td>1.223399</td>\n",
       "      <td>-1.216333</td>\n",
       "      <td>0.021986</td>\n",
       "      <td>1.278536</td>\n",
       "      <td>1.612297</td>\n",
       "      <td>-0.166833</td>\n",
       "      <td>-0.593462</td>\n",
       "      <td>0.301107</td>\n",
       "      <td>2.712527</td>\n",
       "      <td>1.290154</td>\n",
       "      <td>-0.430249</td>\n",
       "      <td>-0.146387</td>\n",
       "      <td>1.748337</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1297</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.285095</td>\n",
       "      <td>-0.794032</td>\n",
       "      <td>-0.377042</td>\n",
       "      <td>-1.311058</td>\n",
       "      <td>1.680782</td>\n",
       "      <td>-2.979678</td>\n",
       "      <td>0.931923</td>\n",
       "      <td>-0.036470</td>\n",
       "      <td>1.246904</td>\n",
       "      <td>2.238842</td>\n",
       "      <td>-0.385095</td>\n",
       "      <td>-1.215255</td>\n",
       "      <td>-0.517741</td>\n",
       "      <td>1.098342</td>\n",
       "      <td>2.130844</td>\n",
       "      <td>1.620444</td>\n",
       "      <td>2.678919</td>\n",
       "      <td>0.954563</td>\n",
       "      <td>1.620180</td>\n",
       "      <td>-0.824253</td>\n",
       "      <td>0.238250</td>\n",
       "      <td>1.609069</td>\n",
       "      <td>0.249393</td>\n",
       "      <td>0.070391</td>\n",
       "      <td>0.624146</td>\n",
       "      <td>-0.327275</td>\n",
       "      <td>-0.246974</td>\n",
       "      <td>-0.003443</td>\n",
       "      <td>0.761711</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1298</td>\n",
       "      <td>3</td>\n",
       "      <td>0.439772</td>\n",
       "      <td>1.450186</td>\n",
       "      <td>-0.241914</td>\n",
       "      <td>2.923996</td>\n",
       "      <td>-1.004166</td>\n",
       "      <td>0.301607</td>\n",
       "      <td>0.177239</td>\n",
       "      <td>0.969418</td>\n",
       "      <td>0.954837</td>\n",
       "      <td>2.354444</td>\n",
       "      <td>-0.549820</td>\n",
       "      <td>-0.774773</td>\n",
       "      <td>2.378162</td>\n",
       "      <td>-0.121109</td>\n",
       "      <td>0.504877</td>\n",
       "      <td>0.348777</td>\n",
       "      <td>0.228840</td>\n",
       "      <td>-2.703182</td>\n",
       "      <td>0.947521</td>\n",
       "      <td>0.459554</td>\n",
       "      <td>-0.162127</td>\n",
       "      <td>1.551671</td>\n",
       "      <td>-0.700785</td>\n",
       "      <td>1.417121</td>\n",
       "      <td>2.149805</td>\n",
       "      <td>-0.674132</td>\n",
       "      <td>-0.598594</td>\n",
       "      <td>1.976982</td>\n",
       "      <td>0.507423</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1299</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.248606</td>\n",
       "      <td>-0.001547</td>\n",
       "      <td>-1.167490</td>\n",
       "      <td>2.092917</td>\n",
       "      <td>0.929471</td>\n",
       "      <td>5.875385</td>\n",
       "      <td>1.015741</td>\n",
       "      <td>-0.210266</td>\n",
       "      <td>-0.322063</td>\n",
       "      <td>-0.111683</td>\n",
       "      <td>0.561311</td>\n",
       "      <td>-1.648200</td>\n",
       "      <td>0.121120</td>\n",
       "      <td>-0.486385</td>\n",
       "      <td>0.272555</td>\n",
       "      <td>-0.132118</td>\n",
       "      <td>4.098165</td>\n",
       "      <td>-0.520020</td>\n",
       "      <td>0.782822</td>\n",
       "      <td>-0.908288</td>\n",
       "      <td>1.449970</td>\n",
       "      <td>0.951928</td>\n",
       "      <td>0.267987</td>\n",
       "      <td>-0.721284</td>\n",
       "      <td>4.247547</td>\n",
       "      <td>0.855840</td>\n",
       "      <td>-0.320104</td>\n",
       "      <td>0.817741</td>\n",
       "      <td>-1.274743</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1300 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0         4  -0.623405   1.157472   0.978446   3.951788   0.918710  -0.742102   \n",
       "1         1  -3.017604   0.962947   1.971217  -2.183554  -2.475973  -4.056342   \n",
       "2         1  -1.814985  -2.619374  -0.777541   1.921058  -0.550762   1.097364   \n",
       "3         3   0.253860   0.276528   1.124507  -0.522192   1.223016  -0.521033   \n",
       "4         4  -1.119872   0.293963   0.421747   4.960528  -1.371113  -1.611126   \n",
       "...     ...        ...        ...        ...        ...        ...        ...   \n",
       "1295      1  -0.791242   0.947756   0.332781   2.080631  -1.414817   2.668325   \n",
       "1296      0   0.251327   0.304282  -0.490810   0.704915  -0.440763   3.088168   \n",
       "1297      3  -2.285095  -0.794032  -0.377042  -1.311058   1.680782  -2.979678   \n",
       "1298      3   0.439772   1.450186  -0.241914   2.923996  -1.004166   0.301607   \n",
       "1299      4  -1.248606  -0.001547  -1.167490   2.092917   0.929471   5.875385   \n",
       "\n",
       "      feature_7  feature_8  feature_9  feature_10  feature_11  feature_12  \\\n",
       "0     -1.764838   1.959709  -0.430550    1.712772   -0.859864    0.858919   \n",
       "1      0.148193   0.529200  -0.692605   -0.036681    0.541161   -0.094085   \n",
       "2      0.453402   0.426274   1.081457    1.688545    0.469907   -0.556450   \n",
       "3      1.267149   0.400720  -0.480437   -0.540828   -1.385455   -0.310562   \n",
       "4     -0.271988   3.291267   0.329875    1.025807    0.464537    1.570395   \n",
       "...         ...        ...        ...         ...         ...         ...   \n",
       "1295   1.109606  -0.559478   0.891290    0.964982   -1.742831   -0.096719   \n",
       "1296  -2.379913  -0.172848   1.768220    1.124578    1.252319    0.792092   \n",
       "1297   0.931923  -0.036470   1.246904    2.238842   -0.385095   -1.215255   \n",
       "1298   0.177239   0.969418   0.954837    2.354444   -0.549820   -0.774773   \n",
       "1299   1.015741  -0.210266  -0.322063   -0.111683    0.561311   -1.648200   \n",
       "\n",
       "      feature_13  feature_14  feature_15  feature_16  feature_17  feature_18  \\\n",
       "0       1.633362    0.185770    1.232641   -1.126940   -0.559032    1.944720   \n",
       "1      -2.196636    0.500172   -0.075832   -1.428413   -2.433943    0.930740   \n",
       "2       0.511860   -0.820182    0.165401    0.971337    1.467646   -1.848128   \n",
       "3       0.668803   -1.082398   -1.223218    0.187642    1.001025   -0.867119   \n",
       "4       3.063703    0.451820    1.708836   -1.790733   -2.399010   -1.059323   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1295    0.975724    1.335518   -0.061370    0.162321    0.599685    1.782970   \n",
       "1296   -1.533918   -1.778728    0.485011   -0.825671    1.223399   -1.216333   \n",
       "1297   -0.517741    1.098342    2.130844    1.620444    2.678919    0.954563   \n",
       "1298    2.378162   -0.121109    0.504877    0.348777    0.228840   -2.703182   \n",
       "1299    0.121120   -0.486385    0.272555   -0.132118    4.098165   -0.520020   \n",
       "\n",
       "      feature_19  feature_20  feature_21  feature_22  feature_23  feature_24  \\\n",
       "0      -0.758760    0.816431    1.126701    1.245197   -0.349880    1.193921   \n",
       "1       0.374944   -1.863622    1.710424   -0.266497    1.327070    1.646004   \n",
       "2       1.346273   -0.846622    0.296170   -1.147455   -0.289827    0.043222   \n",
       "3      -0.313673    0.596596   -0.592904   -0.490817    0.365089   -0.612431   \n",
       "4      -0.059301   -1.674795    0.511317    1.838477    1.946403    0.495256   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1295   -0.172168   -1.074772    0.589416    0.874111    0.341586   -1.272205   \n",
       "1296    0.021986    1.278536    1.612297   -0.166833   -0.593462    0.301107   \n",
       "1297    1.620180   -0.824253    0.238250    1.609069    0.249393    0.070391   \n",
       "1298    0.947521    0.459554   -0.162127    1.551671   -0.700785    1.417121   \n",
       "1299    0.782822   -0.908288    1.449970    0.951928    0.267987   -0.721284   \n",
       "\n",
       "      feature_26  feature_27  feature_28  feature_29  feature_30  \\\n",
       "0      -0.030706   -1.376937   -0.876525   -1.303575   -0.685853   \n",
       "1      -4.494677   -0.890755   -0.290291   -0.411538   -2.506186   \n",
       "2       1.512044    0.193947    0.273527    0.191259   -0.336440   \n",
       "3       0.385040   -0.317562    0.570157    0.224122    0.808667   \n",
       "4      -1.412096    0.891830    1.858374    0.998611    0.720807   \n",
       "...          ...         ...         ...         ...         ...   \n",
       "1295    1.240317   -0.210570    0.393873    0.936976    0.782123   \n",
       "1296    2.712527    1.290154   -0.430249   -0.146387    1.748337   \n",
       "1297    0.624146   -0.327275   -0.246974   -0.003443    0.761711   \n",
       "1298    2.149805   -0.674132   -0.598594    1.976982    0.507423   \n",
       "1299    4.247547    0.855840   -0.320104    0.817741   -1.274743   \n",
       "\n",
       "     categorical_feature_1  \n",
       "0                        C  \n",
       "1                        A  \n",
       "2                        A  \n",
       "3                        B  \n",
       "4                        C  \n",
       "...                    ...  \n",
       "1295                     A  \n",
       "1296                     B  \n",
       "1297                     C  \n",
       "1298                     C  \n",
       "1299                     C  \n",
       "\n",
       "[1300 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop(\"feature_25\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to the train/test split!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training set and test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the most important parts. We will now split the dataset using a common test size ratio of 20% so our models can train on one set of the data and test on the other new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Feature Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though feature scaling is mostly important in algorithms that are distance based and require Euclidean Distance, applying it on our logistical regression model and random forest model will do no harm and might even improve the training performance and therefore, the final predictions. So we'll perform feature scaling nevertheless. However, we have to be careful when scaling our features. We will scale all the features apart from the dummy variables representing our categorial features. Scaling the encoded features will distort our dataset and result in catastrophic results. We also have to ensure that we apply only the transform method to the test set so we don't get a new scalar. We want to use the same scalar as the one on the training set. See the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:]) \n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have preprocessed the data, we can go ahead and analyze the dataset using our base models! We firstly start with Logistic Regression and perform the usual steps to predict our values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced', multi_class='ovr', random_state=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(random_state = 0, multi_class='ovr', class_weight = 'balanced')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Logistic Regression is inherently a binary classifier, we have to specify the multi-class parameter to \"ovr\", or one vs rest. In one-vs-rest logistic regression (OVR) a separate model is trained for each class predicted whether an observation is in that class or not (thus making it a binary classification problem). It assumes that each classification problem (e.g. class 0 or not) is independent. Furthermore, we specify class_weight as 'balanced' to automatically adjust weights inversely proportional to class frequencies in the input data. This will give us a more accurate representation of our model's performance and take unbalanced classes into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [4 4]\n",
      " [1 2]\n",
      " [4 4]\n",
      " [2 2]\n",
      " [3 4]\n",
      " [3 3]\n",
      " [3 1]\n",
      " [0 1]\n",
      " [3 3]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [3 4]\n",
      " [3 1]\n",
      " [0 0]\n",
      " [2 1]\n",
      " [4 4]\n",
      " [2 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [4 3]\n",
      " [1 2]\n",
      " [2 0]\n",
      " [4 3]\n",
      " [3 3]\n",
      " [1 2]\n",
      " [1 4]\n",
      " [0 1]\n",
      " [1 4]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [3 3]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 3]\n",
      " [3 4]\n",
      " [3 3]\n",
      " [3 1]\n",
      " [2 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [3 3]\n",
      " [4 4]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [4 4]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [4 4]\n",
      " [0 1]\n",
      " [3 3]\n",
      " [4 4]\n",
      " [4 1]\n",
      " [1 1]\n",
      " [3 4]\n",
      " [0 0]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [4 3]\n",
      " [0 0]\n",
      " [2 4]\n",
      " [3 4]\n",
      " [3 4]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [2 1]\n",
      " [3 3]\n",
      " [3 1]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [2 1]\n",
      " [2 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [3 4]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [3 3]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 4]\n",
      " [1 1]\n",
      " [4 3]\n",
      " [4 3]\n",
      " [0 4]\n",
      " [4 4]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [4 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [2 3]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 4]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [4 4]\n",
      " [3 3]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 3]\n",
      " [2 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 3]\n",
      " [3 3]\n",
      " [3 4]\n",
      " [3 4]\n",
      " [0 1]\n",
      " [4 4]\n",
      " [3 3]\n",
      " [0 0]\n",
      " [3 4]\n",
      " [0 0]\n",
      " [4 4]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [4 3]\n",
      " [1 4]\n",
      " [1 1]\n",
      " [3 3]\n",
      " [2 2]\n",
      " [4 1]\n",
      " [4 3]\n",
      " [4 3]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 4]\n",
      " [0 0]\n",
      " [3 0]\n",
      " [2 3]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 3]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 4]\n",
      " [2 3]\n",
      " [0 0]\n",
      " [0 3]\n",
      " [2 2]\n",
      " [4 4]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [3 3]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [4 1]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [3 3]\n",
      " [4 4]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 4]\n",
      " [3 4]\n",
      " [3 0]\n",
      " [2 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [2 1]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [4 4]\n",
      " [0 0]\n",
      " [4 4]\n",
      " [3 3]\n",
      " [4 1]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [3 4]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [3 3]\n",
      " [0 1]\n",
      " [4 1]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [4 3]\n",
      " [2 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [2 4]\n",
      " [3 4]\n",
      " [2 2]\n",
      " [2 4]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [3 3]\n",
      " [3 4]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [3 3]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array above compares our predictions with real data and is meant to provide us with an idea of our model's accuracy. The first column represents our predictions and the second column the actual values. It seems that most labels were correctly predicted. We now progress to the next stage where we will evaluate this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have unbalanced classes, it's useful to analyze the perfomance not through direct classification accuracy, but through a confusion matrix so we can gain more insights as to which predictions were correct more often than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix):\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(matrix)\n",
    "    fig.colorbar(cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39  4  3  3  1]\n",
      " [14 54  9  7  5]\n",
      " [ 2  9 12  2  0]\n",
      " [ 3  0  4 21  9]\n",
      " [ 1  7  3 15 33]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(cm, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this confusion matrix, we notice our results are not that great. It seems that the label \"1\" was predicted correctly with the highest accuracy, as we hinted in the beginning of this notebook. This could be due to the fact that our classes are still unbalanced (even after setting class_weight to 'balanced'). This implied that accuracy is definitely not the best metric for performance in a dataset like this. For now we will just take this into consideration and evaluate our model in a more accurate way using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57692308 0.65384615 0.54807692 0.60576923 0.63461538 0.60576923\n",
      " 0.56730769 0.625      0.55769231 0.55769231]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs = -1)\n",
    "print(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5932692307692307\n"
     ]
    }
   ],
   "source": [
    "print(accuracies.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a 10-fold CV yields an average accuracy of 59.3%. However, since we know that we have an unbalanced dataset, we will evaluate our model using other metrics such as precision, recall and the f1 score to provide us with a more accurate representation of our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6115384615384616"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "precision_score(y_test, y_pred, average = \"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation, the \"micro\" parameter calculates metrics globally by counting the total true positives, false negatives and false positives. This takes label imbalance into account rather than using the \"macro\" parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6115384615384616"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred, average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6115384615384616"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average = \"micro\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6115384615384616"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred, average = \"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our metrics, our base model's accuracy through cross validation is approximately 59.3%, with a precision, recall and f1_score of 61.2%. The reason for the same precision, recall and f1 scores is because we used the micro averaging scheme and thus the number of false positives always equals the number of false negatives in this case. Since micro averaging does not distinguish between different classes and then just averages their metric scores, this averaging scheme is not prone to inaccurate values due to an unequally distributed test set (e.g. class '1' in our case which contains the majority of the samples). This is why we prefer this scheme over the macro averaging scheme. Besides micro averaging, one might also consider weighted averaging in case of an unequally distributed data set. But for the purpose of this analysis, we will stick to the micro averaging scheme throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we have seen that we have a dataset that is not balanced. This caused us to modify the class weights and set class_weight as 'balanced'. However, our accuracy was still not great (even though it was lower, it was probably a more realistic representation of model accuracy had we only used default parameters). This caused us to focus our attention on other metrics, such as precision, recall and f1_score. After applying logistic regression, we notice that there is definitely room for improvement. Therefore, we progress to the next stage where we now implement the random forest classifier. We hypothesize that a random forest classifier will perform better than Logistic Regression since it is known to perform better with higher-dimensional data. We will repeat the same procedure as before and evaluate our performance using our usual metrics, taking into account unbalanced labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier_2 = RandomForestClassifier(n_estimators = 10, criterion = \"entropy\", random_state = 0)\n",
    "classifier_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [1 1]\n",
      " [1 4]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [3 4]\n",
      " [1 1]\n",
      " [4 2]\n",
      " [4 4]\n",
      " [1 2]\n",
      " [4 4]\n",
      " [1 2]\n",
      " [4 4]\n",
      " [1 3]\n",
      " [3 1]\n",
      " [1 1]\n",
      " [1 3]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [3 3]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [3 3]\n",
      " [3 3]\n",
      " [1 2]\n",
      " [1 4]\n",
      " [1 1]\n",
      " [1 4]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [3 3]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 3]\n",
      " [4 4]\n",
      " [4 3]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 3]\n",
      " [4 4]\n",
      " [1 3]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [4 4]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [4 4]\n",
      " [0 1]\n",
      " [4 3]\n",
      " [4 4]\n",
      " [4 1]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [0 0]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [4 3]\n",
      " [0 0]\n",
      " [1 4]\n",
      " [1 4]\n",
      " [3 4]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [3 3]\n",
      " [3 1]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 3]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 4]\n",
      " [1 1]\n",
      " [3 3]\n",
      " [1 3]\n",
      " [0 4]\n",
      " [1 4]\n",
      " [4 0]\n",
      " [0 0]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [4 2]\n",
      " [3 2]\n",
      " [3 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 3]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 4]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [3 4]\n",
      " [0 3]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 3]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 3]\n",
      " [1 3]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [3 3]\n",
      " [0 0]\n",
      " [1 4]\n",
      " [0 0]\n",
      " [4 4]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 3]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [3 3]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [4 3]\n",
      " [3 3]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 4]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [3 3]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 3]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [4 4]\n",
      " [1 3]\n",
      " [4 0]\n",
      " [3 3]\n",
      " [1 2]\n",
      " [0 4]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 3]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [3 4]\n",
      " [4 1]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [4 1]\n",
      " [1 3]\n",
      " [4 4]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [1 4]\n",
      " [0 4]\n",
      " [4 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [4 4]\n",
      " [1 0]\n",
      " [1 4]\n",
      " [3 3]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [3 4]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [3 3]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [4 3]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [4 4]\n",
      " [3 4]\n",
      " [1 2]\n",
      " [1 4]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [3 3]\n",
      " [4 4]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [3 3]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [4 1]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_2 = classifier_2.predict(X_test)\n",
    "print(np.concatenate((y_pred_2.reshape(len(y_pred_2),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35 10  1  1  3]\n",
      " [ 9 72  0  4  4]\n",
      " [ 0 18  4  1  2]\n",
      " [ 4 11  0 17  5]\n",
      " [ 3 13  0  6 37]]\n"
     ]
    }
   ],
   "source": [
    "cm_2 = confusion_matrix(y_test, y_pred_2)\n",
    "print(cm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(cm_2, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see by the confusion matrix that the class of '1' still dominates the other classes. While this does look similar to our logistic regression classfier, we will only know for sure after evaluating our performance through cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.625      0.64423077 0.57692308 0.65384615 0.70192308 0.69230769\n",
      " 0.65384615 0.66346154 0.63461538 0.61538462]\n"
     ]
    }
   ],
   "source": [
    "accuracies_2 = cross_val_score(estimator = classifier_2, X = X_train, y = y_train, cv = 10, n_jobs = -1)\n",
    "print(accuracies_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6461538461538462\n"
     ]
    }
   ],
   "source": [
    "print(accuracies_2.mean()) #Higher accuracy than the Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6346153846153846"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred_2, average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6346153846153846"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred_2, average = \"micro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6346153846153846"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred_2, average = \"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we have noticed that random forest performs better than logistic regression, with a higher accuracy of 64.6% and more importantly,  higher precision, recall and f1 scores of 63.46% compared to Logistic regression. This leads to a conclusion that random forest performs better than logistic regression on this dataset! However, there is still room for improvement. So we will apply grid search to fine tune our hyperparameters and conclude which is the best model with the best parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have already noticed that accuracy is not the best measure, we will turn our focus on optimizing the f1 score instead, since it takes both precision and recall into account. We have already seen that random forest performs better than logistic regression. Thus we will only tune the hyperparameters of the random forest in order to reduce computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [{\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [10, 50, 200, 300]\n",
    "}]\n",
    "grid_search_2 = GridSearchCV(estimator = classifier_2, param_grid = param_grid, scoring = \"f1_micro\", n_jobs = -1)\n",
    "grid_search_2 = grid_search_2.fit(X_train, y_train)  \n",
    "best_score_f1 = grid_search_2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6730769230769231\n"
     ]
    }
   ],
   "source": [
    "print(best_score_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 80, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "best_parameters = grid_search_2.best_params_\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that fine-tuning our hyperparamters results in higher f1_score! Thus we can assert that the best model between the two classifiers is \n",
    "a random forest classifier with parameters {'bootstrap': True, 'max_depth': 80, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 50}.\n",
    "However, our analysis is not over yet. We now ask ourselves if there is even a better way to optimize our models? We will focus on that in the penultimate section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SMOTE to mitigate the effects of unbalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the random forest classifier performs better than Logistic Regression. But how can we further improve our model?\n",
    "After evaluating other options, we will turn to re-sampling our data, and use SMOTE, or Synthetic Minority Over-Sampling Technique, which is an oversampling technique designed to increase the samples of the minority class. This approach is effective because new synthetic examples from the minority class are created that are plausible, that is, that are relatively close in the feature space to existing examples from the minority class. We will first import SMOTE from the imblearn.over_sampling library and perform the usual steps and compare the final results on the new set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/aryan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/aryan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/aryan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/aryan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/aryan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/aryan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(sampling_strategy = 'auto', random_state=27)\n",
    "X_train_smote, y_train_smote = sm.fit_sample(X_train, y_train) #It's important to generate the new samples only in the training set to ensure our model generalizes well to unseen data.\n",
    "\n",
    "#Let's now fit our classifiers over our updated dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train_smote) !=  len(X_train)\n",
    "assert len(y_train_smote) != len(y_train) #confirming that we have a resampled dataset with synthetic values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_lr = LogisticRegression(random_state = 0, multi_class='ovr', class_weight = 'balanced').fit(X_train_smote, y_train_smote)\n",
    "smote_pred_lr = smote_lr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_slr = confusion_matrix(y_test, smote_pred_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38  4  3  3  2]\n",
      " [14 41 21  8  5]\n",
      " [ 1  7 15  2  0]\n",
      " [ 5  1  3 20  8]\n",
      " [ 1  6  5 15 32]]\n"
     ]
    }
   ],
   "source": [
    "print(cm_slr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(cm_slr, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice our confusion matrix looks a lot better after applying SMOTE. The predictions look a lot more balanced, with the correct predictions not only dominated by the 1's. This is because our dataset is now a lot more balanced in terms of the weights of the classes. We can now use accuracy as a metric with more confidence this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62162162 0.59459459 0.56756757 0.6        0.64864865 0.6576087\n",
      " 0.67391304 0.74456522 0.67391304 0.69021739]\n"
     ]
    }
   ],
   "source": [
    "accuracies_slr = cross_val_score(estimator = smote_lr, X = X_train_smote, y = y_train_smote, cv = 10, n_jobs = -1)\n",
    "print(accuracies_slr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.647264982373678"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_slr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5615384615384615"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, smote_pred_lr, average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5615384615384615"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, smote_pred_lr , average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5615384615384615"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, smote_pred_lr , average = \"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing our accuracy with the original logistic regression model, we notice an improvement in the average accuracy after cross validation from 59.3% to 64.7%. However, our precision, recall and f1 scores have taken a hit and now reduced from 61.2% to 56.15%.\n",
    "We will now see how our random forest classifier performs under this updated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_rf = RandomForestClassifier(n_estimators = 10, criterion = \"entropy\", random_state = 0).fit(X_train_smote, y_train_smote)\n",
    "smote_pred_rf = smote_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34  7  2  4  3]\n",
      " [12 62  8  4  3]\n",
      " [ 1 16  6  2  0]\n",
      " [ 3  3  1 21  9]\n",
      " [ 3 10  0 11 35]]\n"
     ]
    }
   ],
   "source": [
    "cm_rfc = confusion_matrix(y_test, smote_pred_rf)\n",
    "print(cm_rfc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAECCAYAAADXWsr9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAJ4klEQVR4nO3dz2tdBR6G8fc1zVBbB6SMC2nKtAspU4RRCEUoboqL+gMFVwq6ErKZQguC6NI/QOnGTVFxQFEEXYg4SEGLCI5a2yq2qVDEoUWxUySoLVHSvLPIhem0ndx723Nycvp9PhBI0nDuS8yTc+9NPHESAbi+3dD1AADtI3SgAEIHCiB0oABCBwogdKCAXoRue5ftb2yftP1013uGsf2y7TO2v+56y6hsb7L9oe1Z28ds7+l603Jsr7X9me0vB3uf7XrTqGxP2D5i+92Vus1VH7rtCUkvSLpX0jZJj9re1u2qoV6RtKvrEWNakPRkkr9IukvS31b55/k3STuT/FXSHZJ22b6r402j2iNpdiVvcNWHLmm7pJNJvk3yu6Q3JD3U8aZlJflI0k9d7xhHkh+SHB68/ouWvhA3drvq/8uSXwdvTg5eVv1vf9meknS/pBdX8nb7EPpGSacuevu0VvEX4PXA9mZJd0r6tNslyxvcBT4q6YykA0lW9d6BfZKekrS4kjfah9B9hfet+u/cfWX7JklvSdqb5Oeu9ywnyYUkd0iakrTd9u1db1qO7QcknUnyxUrfdh9CPy1p00VvT0n6vqMt1zXbk1qK/LUkb3e9Z1RJ5iQd1Op/XmSHpAdtf6elh6A7bb+6Ejfch9A/l3Sb7S22/yDpEUnvdLzpumPbkl6SNJvk+a73DGP7Fts3D16/UdI9kk50u2p5SZ5JMpVks5a+jj9I8thK3PaqDz3JgqTdkt7X0hNEbyY51u2q5dl+XdInkrbaPm37ia43jWCHpMe1dJY5Oni5r+tRy7hV0oe2v9LSyeBAkhX7cVXfmP9NFbj+rfozOoBrR+hAAYQOFEDoQAGEDhTQq9Btz3S9YVx929y3vVL/Nnext1ehS+rVf9CBvm3u216pf5sJHUDzWvmFmfXr12fDhg2NH/fcuXNav35948eVpB9//LGV4y4uLuqGG5r/frqwsND4MSUpiZZ+G7Z5bXwepPY+x9LS56ONY7b1OV5cXFSSyw6+po0b27Bhg/bu3dvGoVuzb9++rieM5ezZs11PGNvatWu7njC2tr6htuX8+fNXfD933YECCB0ogNCBAggdKIDQgQIIHSiA0IECCB0ogNCBAggdKIDQgQIIHSiA0IECCB0ogNCBAggdKIDQgQJGCt32Ltvf2D5p++m2RwFo1tDQbU9IekHSvZK2SXrU9ra2hwFozihn9O2STib5NsnvWvoD7g+1OwtAk0YJfaOkUxe9fXrwvv9he8b2IduHzp0719Q+AA0YJfQrXZf2smvgJtmfZDrJdFuXZAZwdUYJ/bSkTRe9PSXp+3bmAGjDKKF/Luk221ts/0HSI5LeaXcWgCYN/QMOSRZs75b0vqQJSS8nOdb6MgCNGekvtSR5T9J7LW8B0BJ+Mw4ogNCBAggdKIDQgQIIHSiA0IECCB0ogNCBAggdKIDQgQIIHSiA0IECCB0ogNCBAggdKIDQgQKcXHadx2u2bt26bN26tfHjtunIkSNdTxjLli1bup4wtrm5ua4njG1hYaHrCWM5f/68Lly4cNkFXTmjAwUQOlAAoQMFEDpQAKEDBRA6UAChAwUQOlAAoQMFEDpQAKEDBRA6UAChAwUQOlAAoQMFEDpQAKEDBRA6UMDQ0G2/bPuM7a9XYhCA5o1yRn9F0q6WdwBo0dDQk3wk6acV2AKgJTxGBwpY09SBbM9ImpGkycnJpg4LoAGNndGT7E8ynWR6zZrGvn8AaAB33YECRvnx2uuSPpG01fZp20+0PwtAk4bex07y6EoMAdAe7roDBRA6UAChAwUQOlAAoQMFEDpQAKEDBRA6UAChAwUQOlAAoQMFEDpQAKEDBRA6UAChAwUQOlAAoQMFOEnzB7XTtwtE3n333V1PGMvs7GzXE8Y2NzfX9YSxzc/Pdz1hbEl86fs4owMFEDpQAKEDBRA6UAChAwUQOlAAoQMFEDpQAKEDBRA6UAChAwUQOlAAoQMFEDpQAKEDBRA6UAChAwUQOlDA0NBtb7L9oe1Z28ds71mJYQCaM8qF3RYkPZnksO0/SvrC9oEkx1veBqAhQ8/oSX5Icnjw+i+SZiVtbHsYgOaM9Rjd9mZJd0r6tI0xANox8jWZbd8k6S1Je5P8fIV/n5E00+A2AA0ZKXTbk1qK/LUkb1/pY5Lsl7R/8PHNXywewFUb5Vl3S3pJ0myS59ufBKBpozxG3yHpcUk7bR8dvNzX8i4ADRp61z3Jx5Iu+xMvAPqD34wDCiB0oABCBwogdKAAQgcKIHSgAEIHCiB0oABCBwogdKAAQgcKIHSgAEIHCiB0oABCBwogdKAAJ81f3m1iYiLr1q1r/Lj4r/n5+a4njO3hhx/uesLYDh8+3PWEsZw6dUrz8/OXXSiGMzpQAKEDBRA6UAChAwUQOlAAoQMFEDpQAKEDBRA6UAChAwUQOlAAoQMFEDpQAKEDBRA6UAChAwUQOlAAoQMFDA3d9lrbn9n+0vYx28+uxDAAzVkzwsf8Jmlnkl9tT0r62PY/kvyz5W0AGjI09CxdPfLXwZuTg5fmrygJoDUjPUa3PWH7qKQzkg4k+bTdWQCaNFLoSS4kuUPSlKTttm+/9GNsz9g+ZPtQG5eQBnD1xnrWPcmcpIOSdl3h3/YnmU4ybV92WWkAHRrlWfdbbN88eP1GSfdIOtH2MADNGeVZ91sl/d32hJa+MbyZ5N12ZwFo0ijPun8l6c4V2AKgJfxmHFAAoQMFEDpQAKEDBRA6UAChAwUQOlAAoQMFEDpQAKEDBRA6UAChAwUQOlAAoQMFEDpQAKEDBRA6UMAol5IqYWpqqusJYzlxon+X7Tt+/HjXE8a2e/furieM5bnnnrvi+zmjAwUQOlAAoQMFEDpQAKEDBRA6UAChAwUQOlAAoQMFEDpQAKEDBRA6UAChAwUQOlAAoQMFEDpQAKEDBRA6UMDIoduesH3E9rttDgLQvHHO6HskzbY1BEB7Rgrd9pSk+yW92O4cAG0Y9Yy+T9JTkhZb3AKgJUNDt/2ApDNJvhjycTO2D9k+lKSxgQCu3Shn9B2SHrT9naQ3JO20/eqlH5Rkf5LpJNO2G54J4FoMDT3JM0mmkmyW9IikD5I81voyAI3h5+hAAWP9SaYkByUdbGUJgNZwRgcKIHSgAEIHCiB0oABCBwogdKAAQgcKIHSgAEIHCiB0oABCBwogdKAAQgcKIHSgAEIHCiB0oABCBwpwG1dstf1vSf9q/MDSnySdbeG4berb5r7tlfq3uc29f05yy6XvbCX0tgwuJT3d9Y5x9G1z3/ZK/dvcxV7uugMFEDpQQN9C39/1gKvQt8192yv1b/OK7+3VY3QAV6dvZ3QAV4HQgQIIHSiA0IECCB0o4D8avitxjJLLsgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(cm_rfc, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77297297 0.75675676 0.71891892 0.80540541 0.83243243 0.84782609\n",
      " 0.875      0.90217391 0.9076087  0.85326087]\n"
     ]
    }
   ],
   "source": [
    "accuracies_rfc = cross_val_score(estimator = smote_rf, X = X_train_smote, y = y_train_smote, cv = 10, n_jobs = -1)\n",
    "print(accuracies_rfc )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8272356051703877"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_rfc.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6076923076923076"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, smote_pred_rf, average = \"micro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6076923076923076"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, smote_pred_rf , average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6076923076923076"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, smote_pred_rf , average = \"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing our classifiers with the updated dataset, it is evident that random forest performs much better than logistical regression! We recorded an accuracy of 82.7% this time which is higher than the SMOTE logistic regression classifier (64.7%). We also recorded a higher f1_score of 60.8% than the logistic classifier (even though when comparing with the fine-tuned parameters of the original random forest model, it is lower). We can now even fine tune our hyper parameters for the random forest model based on accuracy as our scoring criteria with more confidence to once and for all, determine the most optimal model and its set of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_3 = GridSearchCV(estimator = smote_rf, param_grid = param_grid, scoring = \"accuracy\", n_jobs = -1)\n",
    "grid_search_3 = grid_search_3.fit(X_train_smote, y_train_smote)  \n",
    "best_accuracy_smote = grid_search_3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8471544715447153\n"
     ]
    }
   ],
   "source": [
    "print(best_accuracy_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 80, 'max_features': 2, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "best_parameters_smote = grid_search_3.best_params_\n",
    "print(best_parameters_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that this is definitely the highest accuracy we have recorded so far - 84.7%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we may have recorded overall higher accuracies under random forest with SMOTE, the model is obviously far from perfect. There is still a lot of room for improvement and other factors may still need to be considered when analyzing the data. As with many other machine learning projects, there might be other factors that may have been overlooked, some of them which may be beyond the scope of this project. Nevertheless, we can safely conclude that between the two algorithms, random forest performs better. Before applying SMOTE, we noted an imbalance in the weights of the classes and used the sklearn implementation of balanced class weights (for logistic regression), along with generally focusing more on using other metrics such as precision, recall and f1_score to better evaluate our model as accuracy is no longer a very viable metric under such circumstances. Under these metrics, we concluded that a random forest with the following parameters:\n",
    "\n",
    "{'bootstrap': True, 'max_depth': 80, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 50}\n",
    "\n",
    "is indeed the best model when evaluating in terms of f1_scores (with an f1_score of 67.3%). However, we decided that we can still improve the model and evaluate in terms of accuracy by re-sampling our data, in particular by over-sampling our minority classes. This resulted in higher accuracies for both logistic regression and random forest. In particular, random forest, again perfomed better than logistic regression in terms of accuracy, precision, recall and f1_scores! However, even though re-sampling the data resulted in higher accuraces for both models, it resulted in lower f1_scores, precision and recall, albeit not by much as they were still above 50% for all models. Nevertheless, after fine-tuning our hyperparameters through GridSearchCV, we ended up with an accuracy of 84.7%!\n",
    "\n",
    "So to conclude, what is the best model in terms of the highest accuracy? The best model is the random forest classifier with the parameters found after performing grid search on the 'SMOTE'd' training set! This model resulted in an 84.7% accuracy with the following best parameters:\n",
    "\n",
    "## {'bootstrap': True, 'max_depth': 80, 'max_features': 2, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 300}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
